{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOTXcTvAgbGEvmGS5UGWEin",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/femketenharkel/Predicting_Ratings/blob/main/models/clustering/GMM_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GMM clustering"
      ],
      "metadata": {
        "id": "kN_yATx_DiUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "J0EBu2tADmml"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading in the data\n",
        "drive.mount('/content/drive')\n",
        "df_final = pd.read_csv('/content/drive/My Drive/Thesis/Data/df_final.csv')"
      ],
      "metadata": {
        "id": "HpqgPaqXDs7F",
        "outputId": "fa78aa60-cc40-417c-d65f-c31cfa4574ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exclude target variable and unique identifiers for clustering.\n",
        "df = df_final.drop(columns= ['Rating', 'MovieID', 'UserID'])"
      ],
      "metadata": {
        "id": "Y4BuCckoDybL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the variables\n",
        "scaler = MinMaxScaler()\n",
        "features_to_scale = ['Year', 'Month', 'Day', 'Hour', 'Age', 'Release_year',\n",
        "                       'Time_release_to_rating', 'Total_ratings_per_movie',\n",
        "                       'Total_ratings_per_user' ]\n",
        "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])"
      ],
      "metadata": {
        "id": "2Rew9KeMD1-o"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the range for number of clusters\n",
        "n_components_range = range(1, 20, 5)\n",
        "bic_values = []\n",
        "\n",
        "for n_components in n_components_range:\n",
        "    gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
        "    gmm.fit(df)\n",
        "    bic_values.append(gmm.bic(df))\n",
        "\n",
        "# Plotting the elbow graph\n",
        "plt.figure(figsize=(12, 9))\n",
        "plt.plot(n_components_range, bic_values, marker='o')\n",
        "plt.title('Elbow Method for Optimal Number of Clusters (BIC)')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('BIC')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rLObu-L0D4B7",
        "outputId": "767d9e0d-2d72-4d53-d5f4-8df76562802d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3e4c23830503>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_components_range\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianMixture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mbic_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[1;32m    180\u001b[0m         \u001b[0;31m# parameters are validated in fit_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                     \u001b[0mlog_prob_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_resp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_resp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m                     \u001b[0mlower_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_lower_bound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_resp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/mixture/_gaussian_mixture.py\u001b[0m in \u001b[0;36m_m_step\u001b[0;34m(self, X, log_resp)\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mpoint\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \"\"\"\n\u001b[0;32m--> 812\u001b[0;31m         self.weights_, self.means_, self.covariances_ = _estimate_gaussian_parameters(\n\u001b[0m\u001b[1;32m    813\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_resp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_covar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/mixture/_gaussian_mixture.py\u001b[0m in \u001b[0;36m_estimate_gaussian_parameters\u001b[0;34m(X, resp, reg_covar, covariance_type)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0mnk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m     covariances = {\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0;34m\"full\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_estimate_gaussian_covariances_full\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;34m\"tied\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_estimate_gaussian_covariances_tied\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/mixture/_gaussian_mixture.py\u001b[0m in \u001b[0;36m_estimate_gaussian_covariances_full\u001b[0;34m(resp, X, nk, means, reg_covar)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mcovariances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mcovariances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreg_covar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcovariances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/multiarray.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(a, b, out)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_from_c_func_and_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_multiarray_umath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH0trUg-DhlL"
      },
      "outputs": [],
      "source": [
        "# Choose optimal number of clusters based on the plot\n",
        "optimal_n_components = n_components_range[np.argmin(bic_values)]\n",
        "print(f\"Optimal number of clusters: {optimal_n_components}\")\n",
        "\n",
        "# Apply GMM with the optimal number of clusters\n",
        "gmm = GaussianMixture(n_components=optimal_n_components, random_state=42)\n",
        "clusters = gmm.fit_predict(features_normalized)\n",
        "\n",
        "# Add cluster labels back to the DataFrame\n",
        "df['Cluster'] = clusters\n",
        "\n",
        "# Display the DataFrame with clusters\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Poging 2, bic scores"
      ],
      "metadata": {
        "id": "xj5cZqT-anK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. Apply GMM clustering and determine the optimal number of clusters using the elbow method\n",
        "cluster_range = range(1, 11)\n",
        "bic_scores = []\n",
        "\n",
        "for k in cluster_range:\n",
        "    gmm = GaussianMixture(n_components=k, covariance_type='full', random_state=42)\n",
        "    gmm.fit(df_scaled)\n",
        "    bic_scores.append(gmm.bic(df_scaled))\n",
        "\n",
        "# Plot the BIC scores to find the elbow point\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(cluster_range, bic_scores, marker='o')\n",
        "plt.title('Elbow Method For Optimal k using BIC')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('BIC Score')\n",
        "plt.show()\n",
        "\n",
        "# From this plot, determine the optimal number of clusters\n",
        "optimal_clusters = cluster_range[np.argmin(bic_scores)]\n",
        "\n",
        "# 3. Perform clustering again with the optimal number of clusters\n",
        "gmm_optimal = GaussianMixture(n_components=optimal_clusters, covariance_type='full', random_state=42)\n",
        "gmm_optimal.fit(df_scaled)\n",
        "clusters = gmm_optimal.predict(df_scaled)\n",
        "\n",
        "# 4. Evaluate the clustering with the silhouette score\n",
        "silhouette_avg = silhouette_score(df_scaled, clusters)\n",
        "\n",
        "print(f\"Optimal number of clusters: {optimal_clusters}\")\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "\n",
        "# Add the cluster labels to the original dataframe\n",
        "df['Cluster'] = clusters\n",
        "\n",
        "# Print the first few rows of the dataframe with cluster labels\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "yXmrMcPfali7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tyrehfbejhlhjh bic scores\n"
      ],
      "metadata": {
        "id": "cSAyZztzlKnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Define range for number of components (clusters)\n",
        "n_components_range = range(1, 11)\n",
        "\n",
        "# Store BIC values for each number of components\n",
        "bic_values = []\n",
        "\n",
        "# Fit GMM and calculate BIC for each number of components\n",
        "for n_components in n_components_range:\n",
        "    gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)\n",
        "    gmm.fit(df_scaled)\n",
        "    bic_values.append(gmm.bic(df_scaled))\n",
        "\n",
        "# Plot the BIC values\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(n_components_range, bic_values, marker='o')\n",
        "plt.title('Elbow Method for GMM Clustering')\n",
        "plt.xlabel('Number of Components (Clusters)')\n",
        "plt.ylabel('BIC')\n",
        "plt.show()\n",
        "\n",
        "# User inputs the optimal number of clusters based on the elbow plot\n",
        "optimal_n_components = int(input(\"Enter the optimal number of clusters based on the Elbow plot: \"))\n",
        "print(f'Optimal number of clusters: {optimal_n_components}')\n",
        "\n",
        "# Fit GMM with the optimal number of components\n",
        "gmm_optimal = GaussianMixture(n_components=optimal_n_components, covariance_type='full', random_state=42)\n",
        "labels_optimal = gmm_optimal.fit_predict(df_scaled)\n",
        "df['Cluster'] = labels_optimal\n",
        "\n",
        "# Calculate silhouette score for the best performing model\n",
        "sil_score = silhouette_score(df_scaled, labels_optimal)\n",
        "print(f'Silhouette Score for the optimal model: {sil_score:.4f}')\n",
        "\n",
        "# Display the DataFrame with clusters\n",
        "print(df.head())\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(df['Feature1'], df['Feature2'], c=df['Cluster'], cmap='viridis', s=10)\n",
        "plt.title('GMM Clustering with Optimal Number of Clusters')\n",
        "plt.xlabel('Feature1')\n",
        "plt.ylabel('Feature2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jfwPkR_gmr6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 with WSS score"
      ],
      "metadata": {
        "id": "69d3Mgb1rxp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "# Define range for number of components (clusters)\n",
        "n_components_range = range(1, 11)\n",
        "\n",
        "# Store sum of squared distances for each number of components\n",
        "ssd_values = []\n",
        "\n",
        "# Function to calculate sum of squared distances\n",
        "def calculate_ssd(data, labels, centers):\n",
        "    ssd = 0\n",
        "    for i, center in enumerate(centers):\n",
        "        cluster_points = data[labels == i]\n",
        "        ssd += ((cluster_points - center) ** 2).sum()\n",
        "    return ssd\n",
        "\n",
        "# Fit GMM and calculate SSD for each number of components\n",
        "for n_components in n_components_range:\n",
        "    gmm = GaussianMixture(n_components=n_components, covariance_type='full',\n",
        "                          random_state=42)\n",
        "    gmm.fit(df)\n",
        "    labels = gmm.predict(df)\n",
        "    centers = gmm.means_\n",
        "    ssd = calculate_ssd(df, labels, centers)\n",
        "    ssd_values.append(ssd)\n",
        "\n",
        "# Plot the SSD values\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(n_components_range, ssd_values, marker='o')\n",
        "plt.title('Elbow Method for GMM Clustering')\n",
        "plt.xlabel('Number of Components (Clusters)')\n",
        "plt.ylabel('Sum of Squared Distances')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8jbomxs0r0pG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# User inputs the optimal number of clusters based on the elbow plot\n",
        "optimal_n_components = int(input(\"Enter the optimal number of clusters based on the Elbow plot: \"))\n",
        "print(f'Optimal number of clusters: {optimal_n_components}')\n",
        "\n",
        "# Fit GMM with the optimal number of components\n",
        "gmm_optimal = GaussianMixture(n_components=optimal_n_components, covariance_type='full', random_state=42)\n",
        "labels_optimal = gmm_optimal.fit_predict(df)\n",
        "df['Cluster'] = labels_optimal\n",
        "\n",
        "# Calculate silhouette score for the best performing model\n",
        "sil_score = silhouette_score(df, labels_optimal)\n",
        "print(f'Silhouette Score for the optimal model: {sil_score:.4f}')\n",
        "\n",
        "# Display the DataFrame with clusters\n",
        "print(df.head())\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(df['Feature1'], df['Feature2'], c=df['Cluster'], cmap='viridis', s=10)\n",
        "plt.title('GMM Clustering with Optimal Number of Clusters')\n",
        "plt.xlabel('Feature1')\n",
        "plt.ylabel('Feature2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2i964JDJr--K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}