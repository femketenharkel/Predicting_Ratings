{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNjO3D1zl5iH",
        "outputId": "f48a5f39-15ae-44ed-af2f-86bcd284e916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Google colab version\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kSro4HgmAIH"
      },
      "outputs": [],
      "source": [
        "# Load df Google Colab (feature engineered, encoded, scaled/unscaled)\n",
        "import pandas as pd\n",
        "df_sampled_unscaled = pd.read_csv(\"/content/drive/My Drive/Thesis/Data/df_sampled_unscaled.csv\")\n",
        "df_sampled_scaled = pd.read_csv(\"/content/drive/My Drive/Thesis/Data/df_sampled_scaled.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_final = pd.read_csv(\"/content/drive/My Drive/Thesis/Data/df_final_2.csv\")"
      ],
      "metadata": {
        "id": "uxsZp16WHOH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest baseline class imbalance**"
      ],
      "metadata": {
        "id": "B0BsNSM_f7Q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ weighted f1 score as the scoring metric (consistent with XGB)"
      ],
      "metadata": {
        "id": "v9C1bGqQlvH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import KFold, train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load df Google Colab\n",
        "drive.mount('/content/drive')\n",
        "df_final = pd.read_csv(\"/content/drive/My Drive/Thesis/Data/df_final_2.csv\")\n",
        "\n",
        "df_final = df_final.astype(np.int32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYZ-svpugQQW",
        "outputId": "150090ef-3abe-4452-ed00-afbb92f40308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, f1_score, accuracy_score, precision_score, recall_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "import joblib"
      ],
      "metadata": {
        "id": "_XhaNP2PK68-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature and target\n",
        "X = df_final.drop(columns=['Rating'])\n",
        "y = df_final['Rating']\n",
        "\n",
        "# Split data 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    stratify=y, random_state=42)\n",
        "\n",
        "# Define the parameter grid for hyperparameter optimization\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 250, 300],\n",
        "    'max_depth': [20, 30, 40, 50],\n",
        "    'min_samples_split': [2,5,10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Define the inner and outer cross-validation strategies\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define the scoring metric\n",
        "f1_weighted_scorer = make_scorer(f1_score, average='weighted')\n"
      ],
      "metadata": {
        "id": "PLy_q_-wK2p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the Randomized Search with class_weight='balanced'\n",
        "random_search = RandomizedSearchCV(estimator=RandomForestClassifier(class_weight='balanced', random_state=42),\n",
        "                                   param_distributions=param_dist, n_iter=15,\n",
        "                                   cv=inner_cv, scoring=f1_weighted_scorer)\n",
        "\n",
        "# Define output lists\n",
        "outer_scores = []\n",
        "val_scores = []\n",
        "best_params_list = []\n",
        "\n",
        "# Perform nested CV\n",
        "for train_idx, val_idx in outer_cv.split(X_train, y_train):\n",
        "    # Split the data into training and validation sets\n",
        "    X_train_fold = X_train.iloc[train_idx]\n",
        "    X_val_fold = X_train.iloc[val_idx]\n",
        "    y_train_fold = y_train.iloc[train_idx]\n",
        "    y_val_fold = y_train.iloc[val_idx]\n",
        "\n",
        "    # Fit the model on the training fold\n",
        "    random_search.fit(X_train_fold, y_train_fold)\n",
        "    best_model = random_search.best_estimator_\n",
        "\n",
        "    # Save the best parameters for this fold\n",
        "    best_params_list.append(random_search.best_params_)\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    val_predictions = best_model.predict(X_val_fold)\n",
        "    val_acc = accuracy_score(y_val_fold, val_predictions)\n",
        "    val_prec = precision_score(y_val_fold, val_predictions, average='weighted')\n",
        "    val_rec = recall_score(y_val_fold, val_predictions, average='weighted')\n",
        "    val_f1 = f1_score(y_val_fold, val_predictions, average='weighted')\n",
        "\n",
        "    # Store validation metrics\n",
        "    val_scores.append((val_acc, val_prec, val_rec, val_f1))\n",
        "\n",
        "    # Store the outer fold score (f1_weighted)\n",
        "    outer_scores.append(random_search.best_score_)\n",
        "\n",
        "# Print the performance of the nested CV\n",
        "print(f\"Nested Cross-Validation F1-score: {np.mean(outer_scores):.2f} ± {np.std(outer_scores):.2f}\")\n",
        "\n",
        "# Print validation set performance for each fold\n",
        "val_scores = np.array(val_scores)\n",
        "print(\"Validation Set Performance:\")\n",
        "print(f\"Accuracy: {val_scores[:, 0].mean():.2f} ± {val_scores[:, 0].std():.2f}\")\n",
        "print(f\"Precision: {val_scores[:, 1].mean():.2f} ± {val_scores[:, 1].std():.2f}\")\n",
        "print(f\"Recall: {val_scores[:, 2].mean():.2f} ± {val_scores[:, 2].std():.2f}\")\n",
        "print(f\"F1 Score: {val_scores[:, 3].mean():.2f} ± {val_scores[:, 3].std():.2f}\")\n",
        "\n",
        "# Print the best parameters found during hyperparameter tuning\n",
        "print(\"Best parameters found during hyperparameter tuning:\")\n",
        "for i, params in enumerate(best_params_list):\n",
        "    print(f\"Fold {i+1}: {params}\")\n",
        "\n",
        "# Define the best parameters found in nested CV\n",
        "best_params = best_params_list[np.argmax(outer_scores)]\n",
        "\n",
        "# Train the best model with class_weight='balanced'\n",
        "best_model = RandomForestClassifier(**best_params, class_weight='balanced', random_state=42)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_predictions = best_model.predict(X_test)\n",
        "print(\"Test set performance for best performing model:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, test_predictions):.2f}\")\n",
        "print(f\"Precision: {precision_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "print(f\"Recall: {recall_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "\n",
        "# Print the parameters for the best-performing model\n",
        "print(\"Parameters for the best performing model:\")\n",
        "print(best_params)\n",
        "\n",
        "# Save the best performing model\n",
        "joblib_file = \"/content/drive/My Drive/Thesis/Models/Baseline_RF_Class_imb.pkl\"\n",
        "joblib.dump(best_model, joblib_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VjVE2rZgAxU",
        "outputId": "da5e3493-04b5-458f-d9be-e2c849197d99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nested Cross-Validation F1-score: 0.40 ± 0.00\n",
            "Validation Set Performance:\n",
            "Accuracy: 0.41 ± 0.00\n",
            "Precision: 0.40 ± 0.00\n",
            "Recall: 0.41 ± 0.00\n",
            "F1 Score: 0.40 ± 0.00\n",
            "Best parameters found during hyperparameter tuning:\n",
            "Fold 1: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 30}\n",
            "Fold 2: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 30}\n",
            "Fold 3: {'n_estimators': 250, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 40}\n",
            "Fold 4: {'n_estimators': 250, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 50}\n",
            "Fold 5: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 30}\n",
            "Test set performance for best performing model:\n",
            "Accuracy: 0.41\n",
            "Precision: 0.40\n",
            "Recall: 0.41\n",
            "F1 Score: 0.41\n",
            "Parameters for the best performing model:\n",
            "{'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 30}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Thesis/Models/Baseline_RF_Class_imb.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **XGBoost baseline, met training output**"
      ],
      "metadata": {
        "id": "NBTD_7oEURTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xgboost"
      ],
      "metadata": {
        "id": "hXxmgFYHUQx-",
        "outputId": "31879f19-5179-4279-f5e0-f4ca3adff368",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "from sklearn.model_selection import KFold, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer"
      ],
      "metadata": {
        "id": "vlrS3QPXUXPk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the data\n",
        "drive.mount('/content/drive')\n",
        "df_final = pd.read_csv('/content/drive/My Drive/Thesis/Data/df_final_2.csv')\n",
        "\n",
        "df_final = df_final.astype(np.int32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0trMssKfjH4",
        "outputId": "f233207d-581f-4d4c-ec6f-5c98c01dc464"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the \"Rating\" column\n",
        "df_final['Rating'] = label_encoder.fit_transform(df_final['Rating'])"
      ],
      "metadata": {
        "id": "xgkDbAgmfeZU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature and target\n",
        "X = df_final.drop(columns=['Rating'])\n",
        "y = df_final['Rating']\n",
        "\n",
        "# Split data 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    stratify=y, random_state=42)\n",
        "\n",
        "# Define the parameter grid for tuning\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 400],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Define the inner and outer cross-validation strategies\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define the scoring metric\n",
        "f1_weighted_scorer = make_scorer(f1_score, average='weighted')"
      ],
      "metadata": {
        "id": "WygaPsD9fWnQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "import joblib\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def evaluate_model(model, X, y, dataset_name=\"\"):\n",
        "    predictions = model.predict(X)\n",
        "    acc = accuracy_score(y, predictions)\n",
        "    prec = precision_score(y, predictions, average='weighted')\n",
        "    rec = recall_score(y, predictions, average='weighted')\n",
        "    f1 = f1_score(y, predictions, average='weighted')\n",
        "\n",
        "    return acc, prec, rec, f1\n",
        "\n",
        "def evaluate_model_print(model, X, y, dataset_name=\"\"):\n",
        "    predictions = model.predict(X)\n",
        "    acc = accuracy_score(y, predictions)\n",
        "    prec = precision_score(y, predictions, average='weighted')\n",
        "    rec = recall_score(y, predictions, average='weighted')\n",
        "    f1 = f1_score(y, predictions, average='weighted')\n",
        "\n",
        "    print(f\"{dataset_name} Set Performance:\")\n",
        "    print(f\"Accuracy: {acc:.2f}\")\n",
        "    print(f\"Precision: {prec:.2f}\")\n",
        "    print(f\"Recall: {rec:.2f}\")\n",
        "    print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "    return acc, prec, rec, f1\n",
        "\n",
        "\n",
        "def print_average_performance(scores, dataset_name=\"\"):\n",
        "    scores = np.array(scores)\n",
        "    print(f\"{dataset_name} Set Performance (Averaged):\")\n",
        "    print(f\"Accuracy: {scores[:, 0].mean():.2f} ± {scores[:, 0].std():.2f}\")\n",
        "    print(f\"Precision: {scores[:, 1].mean():.2f} ± {scores[:, 1].std():.2f}\")\n",
        "    print(f\"Recall: {scores[:, 2].mean():.2f} ± {scores[:, 2].std():.2f}\")\n",
        "    print(f\"F1 Score: {scores[:, 3].mean():.2f} ± {scores[:, 3].std():.2f}\")\n",
        "\n",
        "def print_hyperparameter_performance(cv_results):\n",
        "    print(\"Average F1 scores for all hyperparameters tried:\")\n",
        "    for params, mean_score in zip(cv_results['params'], cv_results['mean_test_score']):\n",
        "        print(f\"Parameters: {params}, Average F1 Score: {mean_score:.2f}\")\n",
        "\n",
        "# Initialize the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=XGBClassifier(tree_method='hist', device='cuda', eval_metric='merror'),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=15,\n",
        "    cv=inner_cv,\n",
        "    scoring=f1_weighted_scorer,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Define output lists\n",
        "outer_scores = []\n",
        "val_scores = []\n",
        "train_scores = []\n",
        "best_params_list = []\n",
        "\n",
        "# Perform nested CV on the training data\n",
        "for train_idx, val_idx in outer_cv.split(X_train, y_train):\n",
        "    # Split the data into training and validation sets\n",
        "    X_train_fold = X_train.iloc[train_idx]\n",
        "    X_val_fold = X_train.iloc[val_idx]\n",
        "    y_train_fold = y_train.iloc[train_idx]\n",
        "    y_val_fold = y_train.iloc[val_idx]\n",
        "\n",
        "    # Fit the model on the training fold\n",
        "    random_search.fit(X_train_fold, y_train_fold)\n",
        "    best_model = random_search.best_estimator_\n",
        "\n",
        "    # Save the best parameters for this fold\n",
        "    best_params_list.append(random_search.best_params_)\n",
        "\n",
        "    # Evaluate on the training set\n",
        "    train_scores.append(evaluate_model(best_model, X_train_fold, y_train_fold, \"Training\"))\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    val_scores.append(evaluate_model(best_model, X_val_fold, y_val_fold, \"Validation\"))\n",
        "\n",
        "    # Store the outer fold score (f1_weighted)\n",
        "    outer_scores.append(random_search.best_score_)\n",
        "\n",
        "# Print the performance of the nested CV\n",
        "print(f\"Nested Cross-Validation F1 Score: {np.mean(outer_scores):.2f} ± {np.std(outer_scores):.2f}\")\n",
        "print(\"\")\n",
        "print(\"Average metrics for all folds:\")\n",
        "# Print averaged training set performance for each fold\n",
        "print_average_performance(train_scores, \"Training\")\n",
        "# Print averaged validation set performance for each fold\n",
        "print_average_performance(val_scores, \"Validation\")\n",
        "\n",
        "# Print the best parameters found during hyperparameter tuning\n",
        "print(\"Best parameters found during hyperparameter tuning:\")\n",
        "for i, params in enumerate(best_params_list):\n",
        "    print(f\"Fold {i+1}: {params}\")\n",
        "\n",
        "# Print average F1 scores for all hyperparameters tried\n",
        "print_hyperparameter_performance(random_search.cv_results_)\n",
        "\n",
        "# Define the best parameters found in nested CV\n",
        "best_params = best_params_list[np.argmax(outer_scores)]\n",
        "\n",
        "# Train best model on the entire training data\n",
        "best_model = XGBClassifier(**best_params, tree_method='hist', device='cuda', eval_metric='merror', random_state=42)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Evaluation of best performing model:\")\n",
        "# Evaluate on the training set\n",
        "evaluate_model_print(best_model, X_train, y_train, \"Training\")\n",
        "# Evaluate on the test set\n",
        "evaluate_model_print(best_model, X_test, y_test, \"Test\")\n",
        "\n",
        "# Print the parameters for the best-performing model\n",
        "print(\"Parameters for the best performing model:\")\n",
        "print(best_params)\n",
        "\n",
        "joblib.dump(best_model, \"/content/drive/My Drive/Thesis/Models/Baseline_XGB_training_info.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvE_XJmMr7ra",
        "outputId": "aad3b599-343c-4695-f986-fb9732ad41fd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nested Cross-Validation F1 Score: 0.41 ± 0.01\n",
            "\n",
            "Average metrics for all folds:\n",
            "Training Set Performance (Averaged):\n",
            "Accuracy: 0.56 ± 0.05\n",
            "Precision: 0.58 ± 0.05\n",
            "Recall: 0.56 ± 0.05\n",
            "F1 Score: 0.55 ± 0.06\n",
            "Validation Set Performance (Averaged):\n",
            "Accuracy: 0.43 ± 0.01\n",
            "Precision: 0.43 ± 0.00\n",
            "Recall: 0.43 ± 0.01\n",
            "F1 Score: 0.41 ± 0.01\n",
            "Best parameters found during hyperparameter tuning:\n",
            "Fold 1: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Fold 2: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Fold 3: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Fold 4: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Fold 5: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.2}\n",
            "Average F1 scores for all hyperparameters tried:\n",
            "Parameters: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.1}, Average F1 Score: 0.38\n",
            "Parameters: {'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.1}, Average F1 Score: 0.36\n",
            "Parameters: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.05}, Average F1 Score: 0.34\n",
            "Parameters: {'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.2}, Average F1 Score: 0.38\n",
            "Parameters: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.1}, Average F1 Score: 0.39\n",
            "Parameters: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.05}, Average F1 Score: 0.38\n",
            "Parameters: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.05}, Average F1 Score: 0.39\n",
            "Parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.2}, Average F1 Score: 0.35\n",
            "Parameters: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.2}, Average F1 Score: 0.36\n",
            "Parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.05}, Average F1 Score: 0.37\n",
            "Parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.2}, Average F1 Score: 0.40\n",
            "Parameters: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.2}, Average F1 Score: 0.39\n",
            "Parameters: {'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.1}, Average F1 Score: 0.36\n",
            "Parameters: {'n_estimators': 400, 'max_depth': 5, 'learning_rate': 0.05}, Average F1 Score: 0.37\n",
            "Parameters: {'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.05}, Average F1 Score: 0.35\n",
            "Evaluation of best performing model:\n",
            "Training Set Performance:\n",
            "Accuracy: 0.61\n",
            "Precision: 0.62\n",
            "Recall: 0.61\n",
            "F1 Score: 0.60\n",
            "Test Set Performance:\n",
            "Accuracy: 0.44\n",
            "Precision: 0.44\n",
            "Recall: 0.44\n",
            "F1 Score: 0.43\n",
            "Parameters for the best performing model:\n",
            "{'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.2}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Thesis/Models/Baseline_XGB_training_info.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Too much information"
      ],
      "metadata": {
        "id": "iEPJbwKksHLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "import joblib\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def evaluate_model(model, X, y, dataset_name=\"\"):\n",
        "    predictions = model.predict(X)\n",
        "    acc = accuracy_score(y, predictions)\n",
        "    prec = precision_score(y, predictions, average='weighted')\n",
        "    rec = recall_score(y, predictions, average='weighted')\n",
        "    f1 = f1_score(y, predictions, average='weighted')\n",
        "\n",
        "    print(f\"{dataset_name} Set Performance:\")\n",
        "    print(f\"Accuracy: {acc:.2f}\")\n",
        "    print(f\"Precision: {prec:.2f}\")\n",
        "    print(f\"Recall: {rec:.2f}\")\n",
        "    print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "    return acc, prec, rec, f1\n",
        "\n",
        "def print_average_performance(scores, dataset_name=\"\"):\n",
        "    scores = np.array(scores)\n",
        "    print(f\"{dataset_name} Set Performance (Averaged):\")\n",
        "    print(f\"Accuracy: {scores[:, 0].mean():.2f} ± {scores[:, 0].std():.2f}\")\n",
        "    print(f\"Precision: {scores[:, 1].mean():.2f} ± {scores[:, 1].std():.2f}\")\n",
        "    print(f\"Recall: {scores[:, 2].mean():.2f} ± {scores[:, 2].std():.2f}\")\n",
        "    print(f\"F1 Score: {scores[:, 3].mean():.2f} ± {scores[:, 3].std():.2f}\")\n",
        "\n",
        "# Initialize the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=XGBClassifier(tree_method='hist', device='cuda', eval_metric='merror'),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=15,\n",
        "    cv=inner_cv,\n",
        "    scoring=f1_weighted_scorer,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Define output lists\n",
        "outer_scores = []\n",
        "val_scores = []\n",
        "train_scores = []\n",
        "best_params_list = []\n",
        "all_param_scores = []\n",
        "\n",
        "# Perform nested CV on the training data\n",
        "for train_idx, val_idx in outer_cv.split(X_train, y_train):\n",
        "    # Split the data into training and validation sets\n",
        "    X_train_fold = X_train.iloc[train_idx]\n",
        "    X_val_fold = X_train.iloc[val_idx]\n",
        "    y_train_fold = y_train.iloc[train_idx]\n",
        "    y_val_fold = y_train.iloc[val_idx]\n",
        "\n",
        "    # Fit the model on the training fold\n",
        "    random_search.fit(X_train_fold, y_train_fold)\n",
        "    best_model = random_search.best_estimator_\n",
        "\n",
        "    # Save the best parameters for this fold\n",
        "    best_params_list.append(random_search.best_params_)\n",
        "\n",
        "    # Evaluate on the training set\n",
        "    train_scores.append(evaluate_model(best_model, X_train_fold, y_train_fold, \"Training\"))\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    val_scores.append(evaluate_model(best_model, X_val_fold, y_val_fold, \"Validation\"))\n",
        "\n",
        "    # Store the outer fold score (f1_weighted)\n",
        "    outer_scores.append(random_search.best_score_)\n",
        "\n",
        "    # Store all parameter configurations and their scores\n",
        "    all_param_scores.append(random_search.cv_results_['mean_test_score'])\n",
        "\n",
        "# Print the performance of the nested CV\n",
        "print(f\"Nested Cross-Validation F1 Score: {np.mean(outer_scores):.2f} ± {np.std(outer_scores):.2f}\")\n",
        "\n",
        "# Print averaged training set performance for each fold\n",
        "print_average_performance(train_scores, \"Training\")\n",
        "\n",
        "# Print averaged validation set performance for each fold\n",
        "print_average_performance(val_scores, \"Validation\")\n",
        "\n",
        "# Print the best parameters found during hyperparameter tuning\n",
        "print(\"Best parameters found during hyperparameter tuning:\")\n",
        "for i, params in enumerate(best_params_list):\n",
        "    print(f\"Fold {i+1}: {params}\")\n",
        "\n",
        "# Print F1 scores of different parameter configurations tried during hyperparameter tuning\n",
        "print(\"F1 scores of different parameter configurations tried during hyperparameter tuning:\")\n",
        "for i, param_scores in enumerate(all_param_scores):\n",
        "    print(f\"Fold {i+1}: {param_scores}\")\n",
        "\n",
        "# Define the best parameters found in nested CV\n",
        "best_params = best_params_list[np.argmax(outer_scores)]\n",
        "\n",
        "# Train best model on the entire training data\n",
        "best_model = XGBClassifier(**best_params, tree_method='hist', device='cuda', eval_metric='merror', random_state=42)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on the training set\n",
        "evaluate_model(best_model, X_train, y_train, \"Training\")\n",
        "\n",
        "# Evaluate on the test set\n",
        "evaluate_model(best_model, X_test, y_test, \"Test\")\n",
        "\n",
        "# Print the parameters for the best-performing model\n",
        "print(\"Parameters for the best performing model:\")\n",
        "print(best_params)\n",
        "\n",
        "joblib.dump(best_model, \"/content/drive/My Drive/Thesis/Models/Baseline_XGB_training_info.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfpE0Tb4jzeU",
        "outputId": "cd956929-ea98-459a-e646-1a0397462ca7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Performance:\n",
            "Accuracy: 0.59\n",
            "Precision: 0.62\n",
            "Recall: 0.59\n",
            "F1 Score: 0.58\n",
            "Validation Set Performance:\n",
            "Accuracy: 0.44\n",
            "Precision: 0.44\n",
            "Recall: 0.44\n",
            "F1 Score: 0.42\n",
            "Training Set Performance:\n",
            "Accuracy: 0.63\n",
            "Precision: 0.65\n",
            "Recall: 0.63\n",
            "F1 Score: 0.62\n",
            "Validation Set Performance:\n",
            "Accuracy: 0.44\n",
            "Precision: 0.44\n",
            "Recall: 0.44\n",
            "F1 Score: 0.42\n",
            "Training Set Performance:\n",
            "Accuracy: 0.59\n",
            "Precision: 0.62\n",
            "Recall: 0.59\n",
            "F1 Score: 0.58\n",
            "Validation Set Performance:\n",
            "Accuracy: 0.44\n",
            "Precision: 0.43\n",
            "Recall: 0.44\n",
            "F1 Score: 0.42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Performance:\n",
            "Accuracy: 0.63\n",
            "Precision: 0.65\n",
            "Recall: 0.63\n",
            "F1 Score: 0.62\n",
            "Validation Set Performance:\n",
            "Accuracy: 0.44\n",
            "Precision: 0.43\n",
            "Recall: 0.44\n",
            "F1 Score: 0.42\n",
            "Training Set Performance:\n",
            "Accuracy: 0.59\n",
            "Precision: 0.61\n",
            "Recall: 0.59\n",
            "F1 Score: 0.58\n",
            "Validation Set Performance:\n",
            "Accuracy: 0.43\n",
            "Precision: 0.43\n",
            "Recall: 0.43\n",
            "F1 Score: 0.42\n",
            "Nested Cross-Validation F1 Score: 0.41 ± 0.00\n",
            "Training Set Performance (Averaged):\n",
            "Accuracy: 0.61 ± 0.02\n",
            "Precision: 0.63 ± 0.02\n",
            "Recall: 0.61 ± 0.02\n",
            "F1 Score: 0.60 ± 0.02\n",
            "Validation Set Performance (Averaged):\n",
            "Accuracy: 0.44 ± 0.00\n",
            "Precision: 0.43 ± 0.00\n",
            "Recall: 0.44 ± 0.00\n",
            "F1 Score: 0.42 ± 0.00\n",
            "Best parameters found during hyperparameter tuning:\n",
            "Fold 1: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Fold 2: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Fold 3: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Fold 4: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Fold 5: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "F1 scores of different parameter configurations tried during hyperparameter tuning:\n",
            "Fold 1: [0.37301592 0.34095213 0.36365212 0.3272793  0.3739654  0.36607758\n",
            " 0.37098425 0.41149011 0.35716719 0.40331979 0.37398946 0.38262002\n",
            " 0.40573951 0.34069195 0.3843966 ]\n",
            "Fold 2: [0.3571455  0.40100269 0.39154809 0.3917039  0.3618924  0.35008092\n",
            " 0.41273906 0.37446675 0.37331812 0.3494333  0.40461985 0.36158868\n",
            " 0.35889492 0.37070005 0.40646265]\n",
            "Fold 3: [0.40719771 0.36678392 0.32645792 0.41246537 0.39784001 0.35240654\n",
            " 0.37324658 0.39015459 0.37466787 0.40644017 0.37465362 0.40079151\n",
            " 0.3763982  0.38501822 0.35879628]\n",
            "Fold 4: [0.39389385 0.37554744 0.35904874 0.34583958 0.41491759 0.38485788\n",
            " 0.34756803 0.39036206 0.40106942 0.37600908 0.36360713 0.37619148\n",
            " 0.37764736 0.3282323  0.39267928]\n",
            "Fold 5: [0.36439613 0.40789108 0.32734742 0.40768457 0.35239481 0.3746823\n",
            " 0.39372217 0.39863452 0.40479213 0.38429088 0.35856552 0.36381443\n",
            " 0.41279849 0.37642533 0.37375026]\n",
            "Training Set Performance:\n",
            "Accuracy: 0.61\n",
            "Precision: 0.62\n",
            "Recall: 0.61\n",
            "F1 Score: 0.60\n",
            "Test Set Performance:\n",
            "Accuracy: 0.44\n",
            "Precision: 0.44\n",
            "Recall: 0.44\n",
            "F1 Score: 0.43\n",
            "Parameters for the best performing model:\n",
            "{'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.2}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Thesis/Models/Baseline_XGB_training_info.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OLD CODE! NOT USED IN THESIS"
      ],
      "metadata": {
        "id": "aNwl1o8nA6Gk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 29/10 Random Forest with hyperparameter tuning (new sample_df)"
      ],
      "metadata": {
        "id": "eCVUmX5Oweh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold, train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "fucRqTrmx86U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature and target\n",
        "X = df_final.drop('Rating', axis=1)\n",
        "y = df_final['Rating']"
      ],
      "metadata": {
        "id": "bLvFWFIbx8Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
      ],
      "metadata": {
        "id": "PP3Yin5oyF-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for hyperparameter optimization\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}"
      ],
      "metadata": {
        "id": "eiTdzgQb2Kkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the inner and outer cross-validation strategies\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "v_VB5FI32MhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=RandomForestClassifier(),\n",
        "                                   param_distributions=param_dist, n_iter=15,\n",
        "                                   cv=inner_cv, scoring='accuracy',\n",
        "                                   random_state=42)\n",
        "\n",
        "# Perform nested cross-validation\n",
        "outer_scores = []\n",
        "val_scores = []\n",
        "\n",
        "for train_idx, val_idx in outer_cv.split(X_train, y_train):\n",
        "    # Split the data into training and validation sets\n",
        "    X_train_fold = X_train.iloc[train_idx]\n",
        "    X_val_fold = X_train.iloc[val_idx]\n",
        "    y_train_fold = y_train.iloc[train_idx]\n",
        "    y_val_fold = y_train.iloc[val_idx]\n",
        "\n",
        "    # Fit the model on the training fold\n",
        "    random_search.fit(X_train_fold, y_train_fold)\n",
        "    best_model = random_search.best_estimator_\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    val_predictions = best_model.predict(X_val_fold)\n",
        "    val_acc = accuracy_score(y_val_fold, val_predictions)\n",
        "    val_prec = precision_score(y_val_fold, val_predictions, average='weighted')\n",
        "    val_rec = recall_score(y_val_fold, val_predictions, average='weighted')\n",
        "    val_f1 = f1_score(y_val_fold, val_predictions, average='weighted')\n",
        "\n",
        "    # Append validation metrics\n",
        "    val_scores.append((val_acc, val_prec, val_rec, val_f1))\n",
        "\n",
        "    # Store the outer fold score (accuracy)\n",
        "    outer_scores.append(random_search.best_score_)\n",
        "\n",
        "# Print the performance of the nested cross-validation\n",
        "print(f\"Nested Cross-Validation Accuracy: {np.mean(outer_scores):.2f} ± {np.std(outer_scores):.2f}\")\n",
        "\n",
        "# Print validation set performance for each fold\n",
        "val_scores = np.array(val_scores)\n",
        "print(\"Validation Set Performance:\")\n",
        "print(f\"Accuracy: {val_scores[:, 0].mean():.2f} ± {val_scores[:, 0].std():.2f}\")\n",
        "print(f\"Precision: {val_scores[:, 1].mean():.2f} ± {val_scores[:, 1].std():.2f}\")\n",
        "print(f\"Recall: {val_scores[:, 2].mean():.2f} ± {val_scores[:, 2].std():.2f}\")\n",
        "print(f\"F1 Score: {val_scores[:, 3].mean():.2f} ± {val_scores[:, 3].std():.2f}\")\n",
        "\n",
        "# Fit the best model found on the entire training set\n",
        "random_search.fit(X_train, y_train)\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_predictions = best_model.predict(X_test)\n",
        "print(\"Test Set Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, test_predictions):.2f}\")\n",
        "print(f\"Precision: {precision_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "print(f\"Recall: {recall_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, test_predictions, average='weighted'):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGqRKHnS1IpG",
        "outputId": "2d7de325-8216-4e49-bb85-ec9c505ae543"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nested Cross-Validation Accuracy: 0.41 ± 0.00\n",
            "Validation Set Performance:\n",
            "Accuracy: 0.41 ± 0.00\n",
            "Precision: 0.42 ± 0.00\n",
            "Recall: 0.41 ± 0.00\n",
            "F1 Score: 0.38 ± 0.00\n",
            "Test Set Performance:\n",
            "Accuracy: 0.42\n",
            "Precision: 0.43\n",
            "Recall: 0.42\n",
            "F1 Score: 0.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 30/10 Random Forest with hyperparameter tuning, sample_df_3010\n",
        "\n"
      ],
      "metadata": {
        "id": "zZrrOYIsKdMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold, train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "8CXJEMGrGBUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_final = pd.read_csv(\"/content/drive/My Drive/Thesis/Data/sample_df_3010.csv\")"
      ],
      "metadata": {
        "id": "-YN_M1q-KrPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature and target\n",
        "X = df_final.drop('Rating', axis=1)\n",
        "y = df_final['Rating']"
      ],
      "metadata": {
        "id": "dXQf_r3EKwdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    stratify=y, random_state=42)"
      ],
      "metadata": {
        "id": "IsZuQxQBK2yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for hyperparameter optimization\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}"
      ],
      "metadata": {
        "id": "swRSySiMK8-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the inner and outer cross-validation strategies\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "8EqtE9QCLA8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=RandomForestClassifier(),\n",
        "                                   param_distributions=param_dist, n_iter=15,\n",
        "                                   cv=inner_cv, scoring='accuracy',\n",
        "                                   random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# Perform nested cross-validation\n",
        "outer_scores = []\n",
        "val_scores = []\n",
        "\n",
        "for train_idx, val_idx in outer_cv.split(X_train, y_train):\n",
        "    # Split the data into training and validation sets\n",
        "    X_train_fold = X_train.iloc[train_idx]\n",
        "    X_val_fold = X_train.iloc[val_idx]\n",
        "    y_train_fold = y_train.iloc[train_idx]\n",
        "    y_val_fold = y_train.iloc[val_idx]\n",
        "\n",
        "    # Fit the model on the training fold\n",
        "    random_search.fit(X_train_fold, y_train_fold)\n",
        "    best_model = random_search.best_estimator_\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    val_predictions = best_model.predict(X_val_fold)\n",
        "    val_acc = accuracy_score(y_val_fold, val_predictions)\n",
        "    val_prec = precision_score(y_val_fold, val_predictions, average='weighted')\n",
        "    val_rec = recall_score(y_val_fold, val_predictions, average='weighted')\n",
        "    val_f1 = f1_score(y_val_fold, val_predictions, average='weighted')\n",
        "\n",
        "    # Append validation metrics\n",
        "    val_scores.append((val_acc, val_prec, val_rec, val_f1))\n",
        "\n",
        "    # Store the outer fold score (accuracy)\n",
        "    outer_scores.append(random_search.best_score_)\n",
        "\n",
        "# Print the performance of the nested cross-validation\n",
        "print(f\"Nested Cross-Validation Accuracy: {np.mean(outer_scores):.2f} ± {np.std(outer_scores):.2f}\")\n",
        "\n",
        "# Print validation set performance for each fold\n",
        "val_scores = np.array(val_scores)\n",
        "print(\"Validation Set Performance:\")\n",
        "print(f\"Accuracy: {val_scores[:, 0].mean():.2f} ± {val_scores[:, 0].std():.2f}\")\n",
        "print(f\"Precision: {val_scores[:, 1].mean():.2f} ± {val_scores[:, 1].std():.2f}\")\n",
        "print(f\"Recall: {val_scores[:, 2].mean():.2f} ± {val_scores[:, 2].std():.2f}\")\n",
        "print(f\"F1 Score: {val_scores[:, 3].mean():.2f} ± {val_scores[:, 3].std():.2f}\")\n",
        "\n",
        "# Fit the best model found on the entire training set\n",
        "random_search.fit(X_train, y_train)\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_predictions = best_model.predict(X_test)\n",
        "print(\"Test Set Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, test_predictions):.2f}\")\n",
        "print(f\"Precision: {precision_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "print(f\"Recall: {recall_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, test_predictions, average='weighted'):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1OnqhvyLJtI",
        "outputId": "54768b1c-1f0d-487f-a47f-8903d40490bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nested Cross-Validation Accuracy: 0.40 ± 0.00\n",
            "Validation Set Performance:\n",
            "Accuracy: 0.41 ± 0.00\n",
            "Precision: 0.42 ± 0.00\n",
            "Recall: 0.41 ± 0.00\n",
            "F1 Score: 0.38 ± 0.00\n",
            "Test Set Performance:\n",
            "Accuracy: 0.41\n",
            "Precision: 0.42\n",
            "Recall: 0.41\n",
            "F1 Score: 0.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2slKv5PUb61g"
      },
      "source": [
        "## Gradient boosting with hyperparameter tuning (-2 features) -> Accuracy 0.46\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfOwssYDb_On"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load df Google Colab (feature engineered, encoded, scaled/unscaled)\n",
        "df_sampled_unscaled = pd.read_csv(\"/content/drive/My Drive/Thesis/Data/df_sampled_unscaled.csv\")\n",
        "df_sampled_scaled = pd.read_csv(\"/content/drive/My Drive/Thesis/Data/df_sampled_scaled.csv\")"
      ],
      "metadata": {
        "id": "SAevHd6ZrOc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = df_sampled_unscaled.copy()\n",
        "df_final = df_final.drop(columns=['Dev_movie_avg',\t'Total_ratings_per_user'])"
      ],
      "metadata": {
        "id": "5sn4jTkhrN5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhzrjZYSR-Nu"
      },
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X = df_final.drop(columns=['Rating'])\n",
        "y = df_final['Rating']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GP55Vcv9cGXH"
      },
      "outputs": [],
      "source": [
        "# Split data into 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTVqY2_bcIUT"
      },
      "outputs": [],
      "source": [
        "# Define evaluation function\n",
        "def evaluate_performance(y_true, y_pred):\n",
        "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.2f}\")\n",
        "    print(f\"Precision: {precision_score(y_true, y_pred, average='macro'):.2f}\")\n",
        "    print(f\"Recall: {recall_score(y_true, y_pred, average='macro'):.2f}\")\n",
        "    print(f\"F1 Score: {f1_score(y_true, y_pred, average='macro'):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZ8nSvgNcJUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c911d2fe-726e-439e-b8b7-f71ebafca7d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nested Cross-Validation Accuracy: 0.46 ± 0.00\n",
            "Best parameters for each fold:\n",
            "{'n_estimators': 50, 'max_depth': 7, 'learning_rate': 0.1}\n",
            "{'n_estimators': 50, 'max_depth': 7, 'learning_rate': 0.1}\n",
            "{'n_estimators': 50, 'max_depth': 7, 'learning_rate': 0.1}\n",
            "{'n_estimators': 50, 'max_depth': 7, 'learning_rate': 0.1}\n",
            "{'n_estimators': 50, 'max_depth': 7, 'learning_rate': 0.1}\n",
            "Test Set Performance:\n",
            "Accuracy: 0.46\n",
            "Precision: 0.46\n",
            "Recall: 0.38\n",
            "F1 Score: 0.40\n"
          ]
        }
      ],
      "source": [
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Define the parameter grid for tuning\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 7, 10, 20]\n",
        "}\n",
        "\n",
        "# Define the inner and outer cross-validation strategies\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=GradientBoostingClassifier(),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=15,\n",
        "    cv=inner_cv,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1  # Utilize all available cores for parallel processing\n",
        ")\n",
        "\n",
        "# Perform nested cross-validation and store best parameters for each fold\n",
        "nested_scores = []\n",
        "best_params = []\n",
        "best_models = []\n",
        "\n",
        "def fit_and_evaluate(train_idx, test_idx):\n",
        "    random_search.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
        "    best_model = random_search.best_estimator_\n",
        "    return random_search.best_score_, random_search.best_params_, best_model\n",
        "\n",
        "results = Parallel(n_jobs=-1)(delayed(fit_and_evaluate)(train_idx, test_idx) for train_idx, test_idx in outer_cv.split(X_train, y_train))\n",
        "\n",
        "for score, params, model in results:\n",
        "    nested_scores.append(score)\n",
        "    best_params.append(params)\n",
        "    best_models.append(model)\n",
        "\n",
        "# Print the performance of the nested cross-validation\n",
        "print(f\"Nested Cross-Validation Accuracy: {np.mean(nested_scores):.2f} ± {np.std(nested_scores):.2f}\")\n",
        "\n",
        "# Print the best parameters found for each fold\n",
        "print(\"Best parameters for each fold:\")\n",
        "for params in best_params:\n",
        "    print(params)\n",
        "\n",
        "# Use the best model found in the outer loop\n",
        "final_best_model = best_models[-1]\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "test_predictions = final_best_model.predict(X_test)\n",
        "print(\"Test Set Performance:\")\n",
        "evaluate_performance(y_test, test_predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BASELINE RANDOM FOREST 10/11, df_final_2**\n",
        "\n",
        "> Good dataframe, not good that gridcv is ran twice!!\n",
        "\n"
      ],
      "metadata": {
        "id": "WCOZqmHflSan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google colab version\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmS816MclWFW",
        "outputId": "7397fa59-cae0-4692-8223-8c2ef1121a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load df Google Colab (feature engineered, encoded, scaled/unscaled)\n",
        "import pandas as pd\n",
        "df_final = pd.read_csv(\"/content/drive/My Drive/Thesis/Data/df_final_2.csv\")\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "df_final.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "9ZKt1Tg7liNw",
        "outputId": "b4d2a14c-57a8-4570-d998-ecc46fe7f6ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   UserID  MovieID  Rating  Age  Year  Month  Day  Hour  Release_year  \\\n",
              "0    5616     3590       3    4  2000      5   24     2          1974   \n",
              "1    4060       21       4    2  2000      8    5    15          1995   \n",
              "2    1125     3273       2    1  2000     11   22    16          2000   \n",
              "3    3410      585       4    3  2000      8   27    22          1995   \n",
              "4    3675     1374       4    3  2000      8   15    18          1982   \n",
              "\n",
              "   Time_release_to_rating  Total_ratings_per_movie  Total_ratings_per_user  \\\n",
              "0                      26                       85                     130   \n",
              "1                       5                     1356                     256   \n",
              "2                       0                      577                     580   \n",
              "3                       5                      418                     731   \n",
              "4                      18                     1448                     849   \n",
              "\n",
              "   Female   Male  Academic/educator  Artist  Clerical/admin  \\\n",
              "0   False   True               True   False           False   \n",
              "1   False   True              False   False           False   \n",
              "2    True  False              False   False           False   \n",
              "3   False   True               True   False           False   \n",
              "4   False   True              False   False           False   \n",
              "\n",
              "   College/grad student  Customer service  Doctor/health care  \\\n",
              "0                 False             False               False   \n",
              "1                 False             False               False   \n",
              "2                  True             False               False   \n",
              "3                 False             False               False   \n",
              "4                 False             False               False   \n",
              "\n",
              "   Executive/managerial  Farmer  Homemaker  K-12 student  Lawyer  \\\n",
              "0                 False   False      False         False   False   \n",
              "1                 False   False      False         False   False   \n",
              "2                 False   False      False         False   False   \n",
              "3                 False   False      False         False   False   \n",
              "4                  True   False      False         False   False   \n",
              "\n",
              "   Other or not specified  Programmer  Retired  Sales/marketing  Scientist  \\\n",
              "0                   False       False    False            False      False   \n",
              "1                   False       False    False            False      False   \n",
              "2                   False       False    False            False      False   \n",
              "3                   False       False    False            False      False   \n",
              "4                   False       False    False            False      False   \n",
              "\n",
              "   Self-employed  Technician/engineer  Tradesman/craftsman  Unemployed  \\\n",
              "0          False                False                False       False   \n",
              "1          False                False                 True       False   \n",
              "2          False                False                False       False   \n",
              "3          False                False                False       False   \n",
              "4          False                False                False       False   \n",
              "\n",
              "   Writer  Action  Adventure  Animation  Children's  Comedy  Crime  \\\n",
              "0   False   False      False      False       False    True  False   \n",
              "1   False    True      False      False       False    True  False   \n",
              "2   False   False      False      False       False   False  False   \n",
              "3   False   False      False      False       False    True  False   \n",
              "4   False    True       True      False       False   False  False   \n",
              "\n",
              "   Documentary  Drama  Fantasy  Film-Noir  Horror  Musical  Mystery  Romance  \\\n",
              "0        False  False    False      False   False    False    False    False   \n",
              "1        False   True    False      False   False    False    False    False   \n",
              "2        False  False    False      False    True    False     True    False   \n",
              "3        False  False    False      False   False    False    False    False   \n",
              "4        False  False    False      False   False    False    False    False   \n",
              "\n",
              "   Sci-Fi  Thriller    War  Western  Favourite_Action  Favourite_Adventure  \\\n",
              "0   False     False  False    False             False                False   \n",
              "1   False     False  False    False             False                False   \n",
              "2   False      True  False    False             False                False   \n",
              "3   False     False  False    False             False                False   \n",
              "4    True     False  False    False             False                False   \n",
              "\n",
              "   Favourite_Animation  Favourite_Children's  Favourite_Comedy  \\\n",
              "0                False                 False             False   \n",
              "1                False                 False             False   \n",
              "2                False                 False             False   \n",
              "3                False                 False             False   \n",
              "4                False                 False             False   \n",
              "\n",
              "   Favourite_Crime  Favourite_Documentary  Favourite_Drama  Favourite_Fantasy  \\\n",
              "0            False                  False            False              False   \n",
              "1            False                  False            False              False   \n",
              "2            False                  False            False              False   \n",
              "3            False                  False            False              False   \n",
              "4            False                  False            False              False   \n",
              "\n",
              "   Favourite_Film-Noir  Favourite_Horror  Favourite_Musical  \\\n",
              "0                False             False              False   \n",
              "1                False             False              False   \n",
              "2                 True             False              False   \n",
              "3                 True             False              False   \n",
              "4                 True             False              False   \n",
              "\n",
              "   Favourite_Mystery  Favourite_Romance  Favourite_Sci-Fi  Favourite_Thriller  \\\n",
              "0              False              False             False               False   \n",
              "1              False              False             False               False   \n",
              "2              False              False             False               False   \n",
              "3              False              False             False               False   \n",
              "4              False              False             False               False   \n",
              "\n",
              "   Favourite_War  Favourite_Western  \n",
              "0          False               True  \n",
              "1          False               True  \n",
              "2          False              False  \n",
              "3          False              False  \n",
              "4          False              False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7e48e71e-6904-4a3d-b20e-19e8527c52a3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>MovieID</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Age</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Day</th>\n",
              "      <th>Hour</th>\n",
              "      <th>Release_year</th>\n",
              "      <th>Time_release_to_rating</th>\n",
              "      <th>Total_ratings_per_movie</th>\n",
              "      <th>Total_ratings_per_user</th>\n",
              "      <th>Female</th>\n",
              "      <th>Male</th>\n",
              "      <th>Academic/educator</th>\n",
              "      <th>Artist</th>\n",
              "      <th>Clerical/admin</th>\n",
              "      <th>College/grad student</th>\n",
              "      <th>Customer service</th>\n",
              "      <th>Doctor/health care</th>\n",
              "      <th>Executive/managerial</th>\n",
              "      <th>Farmer</th>\n",
              "      <th>Homemaker</th>\n",
              "      <th>K-12 student</th>\n",
              "      <th>Lawyer</th>\n",
              "      <th>Other or not specified</th>\n",
              "      <th>Programmer</th>\n",
              "      <th>Retired</th>\n",
              "      <th>Sales/marketing</th>\n",
              "      <th>Scientist</th>\n",
              "      <th>Self-employed</th>\n",
              "      <th>Technician/engineer</th>\n",
              "      <th>Tradesman/craftsman</th>\n",
              "      <th>Unemployed</th>\n",
              "      <th>Writer</th>\n",
              "      <th>Action</th>\n",
              "      <th>Adventure</th>\n",
              "      <th>Animation</th>\n",
              "      <th>Children's</th>\n",
              "      <th>Comedy</th>\n",
              "      <th>Crime</th>\n",
              "      <th>Documentary</th>\n",
              "      <th>Drama</th>\n",
              "      <th>Fantasy</th>\n",
              "      <th>Film-Noir</th>\n",
              "      <th>Horror</th>\n",
              "      <th>Musical</th>\n",
              "      <th>Mystery</th>\n",
              "      <th>Romance</th>\n",
              "      <th>Sci-Fi</th>\n",
              "      <th>Thriller</th>\n",
              "      <th>War</th>\n",
              "      <th>Western</th>\n",
              "      <th>Favourite_Action</th>\n",
              "      <th>Favourite_Adventure</th>\n",
              "      <th>Favourite_Animation</th>\n",
              "      <th>Favourite_Children's</th>\n",
              "      <th>Favourite_Comedy</th>\n",
              "      <th>Favourite_Crime</th>\n",
              "      <th>Favourite_Documentary</th>\n",
              "      <th>Favourite_Drama</th>\n",
              "      <th>Favourite_Fantasy</th>\n",
              "      <th>Favourite_Film-Noir</th>\n",
              "      <th>Favourite_Horror</th>\n",
              "      <th>Favourite_Musical</th>\n",
              "      <th>Favourite_Mystery</th>\n",
              "      <th>Favourite_Romance</th>\n",
              "      <th>Favourite_Sci-Fi</th>\n",
              "      <th>Favourite_Thriller</th>\n",
              "      <th>Favourite_War</th>\n",
              "      <th>Favourite_Western</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5616</td>\n",
              "      <td>3590</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2000</td>\n",
              "      <td>5</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>1974</td>\n",
              "      <td>26</td>\n",
              "      <td>85</td>\n",
              "      <td>130</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4060</td>\n",
              "      <td>21</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2000</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>15</td>\n",
              "      <td>1995</td>\n",
              "      <td>5</td>\n",
              "      <td>1356</td>\n",
              "      <td>256</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1125</td>\n",
              "      <td>3273</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2000</td>\n",
              "      <td>11</td>\n",
              "      <td>22</td>\n",
              "      <td>16</td>\n",
              "      <td>2000</td>\n",
              "      <td>0</td>\n",
              "      <td>577</td>\n",
              "      <td>580</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3410</td>\n",
              "      <td>585</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2000</td>\n",
              "      <td>8</td>\n",
              "      <td>27</td>\n",
              "      <td>22</td>\n",
              "      <td>1995</td>\n",
              "      <td>5</td>\n",
              "      <td>418</td>\n",
              "      <td>731</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3675</td>\n",
              "      <td>1374</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2000</td>\n",
              "      <td>8</td>\n",
              "      <td>15</td>\n",
              "      <td>18</td>\n",
              "      <td>1982</td>\n",
              "      <td>18</td>\n",
              "      <td>1448</td>\n",
              "      <td>849</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e48e71e-6904-4a3d-b20e-19e8527c52a3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7e48e71e-6904-4a3d-b20e-19e8527c52a3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7e48e71e-6904-4a3d-b20e-19e8527c52a3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7aba73a8-08be-4999-8d09-a86cc4557e35\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7aba73a8-08be-4999-8d09-a86cc4557e35')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7aba73a8-08be-4999-8d09-a86cc4557e35 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_final"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold, train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "Qu86mr3ylylx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature and target\n",
        "X = df_final.drop('Rating', axis=1)\n",
        "y = df_final['Rating']\n",
        "\n",
        "# Split data 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    stratify=y, random_state=42)\n",
        "\n",
        "# Define the parameter grid for hyperparameter optimization\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 250, 300],\n",
        "    'max_depth': [20, 30, 40, 50],\n",
        "    'min_samples_split': [2,5,10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Define the inner and outer cross-validation strategies\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "B4Ei-K9Vl1Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turning booleans into numerical values\n",
        "#df_final = np.nan_to_num(df_final)\n",
        "df_final.astype(np.int32)"
      ],
      "metadata": {
        "id": "unl_sSwBGZBr",
        "outputId": "ef9dd424-2009-4eea-ff96-ae01e38e7ba3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5616, 3590,    3, ...,    0,    0,    1],\n",
              "       [4060,   21,    4, ...,    0,    0,    1],\n",
              "       [1125, 3273,    2, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [1100, 3525,    1, ...,    0,    0,    0],\n",
              "       [1971, 2355,    4, ...,    0,    1,    0],\n",
              "       [5136, 1429,    4, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Ranodom Search\n",
        "random_search = RandomizedSearchCV(estimator=RandomForestClassifier(),\n",
        "                                   param_distributions=param_dist, n_iter=15,\n",
        "                                   cv=inner_cv, scoring='accuracy',\n",
        "                                   random_state=42)\n",
        "\n",
        "# Define output lists\n",
        "outer_scores = []\n",
        "val_scores = []\n",
        "best_params_list = []\n",
        "\n",
        "# Perform nested cross-validation\n",
        "for train_idx, val_idx in outer_cv.split(X_train, y_train):\n",
        "    # Split the data into training and validation sets\n",
        "    X_train_fold = X_train.iloc[train_idx]\n",
        "    X_val_fold = X_train.iloc[val_idx]\n",
        "    y_train_fold = y_train.iloc[train_idx]\n",
        "    y_val_fold = y_train.iloc[val_idx]\n",
        "\n",
        "    # Fit the model on the training fold\n",
        "    random_search.fit(X_train_fold, y_train_fold)\n",
        "    best_model = random_search.best_estimator_\n",
        "\n",
        "    # Save the best parameters for this fold\n",
        "    best_params_list.append(random_search.best_params_)\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    val_predictions = best_model.predict(X_val_fold)\n",
        "    val_acc = accuracy_score(y_val_fold, val_predictions)\n",
        "    val_prec = precision_score(y_val_fold, val_predictions, average='weighted')\n",
        "    val_rec = recall_score(y_val_fold, val_predictions, average='weighted')\n",
        "    val_f1 = f1_score(y_val_fold, val_predictions, average='weighted')\n",
        "\n",
        "    # Append validation metrics\n",
        "    val_scores.append((val_acc, val_prec, val_rec, val_f1))\n",
        "\n",
        "    # Store the outer fold score (accuracy)\n",
        "    outer_scores.append(random_search.best_score_)\n",
        "\n",
        "# Print the performance of the nested cross-validation\n",
        "print(f\"Nested Cross-Validation Accuracy: {np.mean(outer_scores):.2f} ± {np.std(outer_scores):.2f}\")\n",
        "\n",
        "# Print validation set performance for each fold\n",
        "val_scores = np.array(val_scores)\n",
        "print(\"Validation Set Performance:\")\n",
        "print(f\"Accuracy: {val_scores[:, 0].mean():.2f} ± {val_scores[:, 0].std():.2f}\")\n",
        "print(f\"Precision: {val_scores[:, 1].mean():.2f} ± {val_scores[:, 1].std():.2f}\")\n",
        "print(f\"Recall: {val_scores[:, 2].mean():.2f} ± {val_scores[:, 2].std():.2f}\")\n",
        "print(f\"F1 Score: {val_scores[:, 3].mean():.2f} ± {val_scores[:, 3].std():.2f}\")\n",
        "\n",
        "# Print the best parameters found during hyperparameter tuning\n",
        "print(\"Best parameters found during hyperparameter tuning:\")\n",
        "for i, params in enumerate(best_params_list):\n",
        "    print(f\"Fold {i+1}: {params}\")\n",
        "\n",
        "# Fit the best model found on the entire training set\n",
        "random_search.fit(X_train, y_train)\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_predictions = best_model.predict(X_test)\n",
        "print(\"Test Set Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, test_predictions):.2f}\")\n",
        "print(f\"Precision: {precision_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "print(f\"Recall: {recall_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "\n",
        "# Print the best parameters of the best-performing model\n",
        "print(\"Best Parameters of the Best-Performing Model:\")\n",
        "print(random_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1DTIy_4l37E",
        "outputId": "8507ec8a-5965-48d7-ec71-a11e75fa4362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nested Cross-Validation Accuracy: 0.41 ± 0.00\n",
            "Validation Set Performance:\n",
            "Accuracy: 0.42 ± 0.00\n",
            "Precision: 0.42 ± 0.00\n",
            "Recall: 0.42 ± 0.00\n",
            "F1 Score: 0.39 ± 0.00\n",
            "Best parameters found during hyperparameter tuning:\n",
            "Fold 1: {'n_estimators': 250, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_depth': 40}\n",
            "Fold 2: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': 30}\n",
            "Fold 3: {'n_estimators': 250, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_depth': 40}\n",
            "Fold 4: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': 30}\n",
            "Fold 5: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': 30}\n",
            "Test Set Performance:\n",
            "Accuracy: 0.42\n",
            "Precision: 0.43\n",
            "Recall: 0.42\n",
            "F1 Score: 0.40\n",
            "Best Parameters of the Best-Performing Model:\n",
            "{'n_estimators': 250, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_depth': 40}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collaborative Filtering, matrix"
      ],
      "metadata": {
        "id": "4ubMaqIb8vSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google colab version\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "df_final = pd.read_csv(\"/content/drive/My Drive/Thesis/Data/df_final_2.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJdsFNZXRKuW",
        "outputId": "220a1651-996d-4087-af9e-fe2a866d9843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImputationMatrixFactorization:\n",
        "    def __init__(self, R, K, alpha, beta, iterations):\n",
        "        self.R = R\n",
        "        self.num_users, self.num_items = R.shape\n",
        "        self.K = K\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.iterations = iterations\n",
        "\n",
        "    def impute_missing_values(self):\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        self.R_imputed = imputer.fit_transform(self.R)\n",
        "\n",
        "    def train(self):\n",
        "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
        "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
        "        self.b_u = np.zeros(self.num_users)\n",
        "        self.b_i = np.zeros(self.num_items)\n",
        "        self.b = np.mean(self.R_imputed[np.where(self.R_imputed != 0)])\n",
        "\n",
        "        self.samples = [\n",
        "            (i, j, self.R_imputed[i, j])\n",
        "            for i in range(self.num_users)\n",
        "            for j in range(self.num_items)\n",
        "            if self.R[i, j] > 0\n",
        "        ]\n",
        "\n",
        "        for i in range(self.iterations):\n",
        "            np.random.shuffle(self.samples)\n",
        "            self.sgd()\n",
        "            mse = self.mse()\n",
        "            if (i+1) % 10 == 0:\n",
        "                print(f\"Iteration: {i+1}; error = {mse:.4f}\")\n",
        "\n",
        "    def mse(self):\n",
        "        xs, ys = self.R.nonzero()\n",
        "        predicted = self.full_matrix()\n",
        "        error = 0\n",
        "        for x, y in zip(xs, ys):\n",
        "            error += (self.R[x, y] - predicted[x, y]) ** 2\n",
        "        return np.sqrt(error)\n",
        "\n",
        "    def sgd(self):\n",
        "        for i, j, r in self.samples:\n",
        "            prediction = self.get_prediction(i, j)\n",
        "            e = (r - prediction)\n",
        "\n",
        "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
        "            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])\n",
        "\n",
        "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
        "            self.Q[j, :] += self.alpha * (e * self.P[i, :] - self.beta * self.Q[j,:])\n",
        "\n",
        "    def get_prediction(self, i, j):\n",
        "        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
        "        return prediction\n",
        "\n",
        "    def full_matrix(self):\n",
        "        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)\n",
        "\n"
      ],
      "metadata": {
        "id": "FDzhuUqMVC2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R_df = df_final.pivot(index='UserID', columns='MovieID', values='Rating')\n",
        "R = R_df.values\n",
        "\n",
        "imf = ImputationMatrixFactorization(R, K=2, alpha=0.01, beta=0.01, iterations=100)\n",
        "imf.impute_missing_values()\n",
        "imf.train()\n",
        "predicted_matrix = imf.full_matrix()\n",
        "print(predicted_matrix)\n",
        "\n",
        "# Evaluation\n",
        "def get_true_pred_ratings(df, predicted_df):\n",
        "    true_ratings = []\n",
        "    predicted_ratings = []\n",
        "    for row in df.itertuples():\n",
        "        user_id = row.UserID\n",
        "        movie_id = row.MovieID\n",
        "        true_ratings.append(row.Rating)\n",
        "        predicted_ratings.append(predicted_df.loc[user_id, movie_id])\n",
        "    return np.array(true_ratings), np.array(predicted_ratings)\n",
        "\n",
        "true_ratings, predicted_ratings = get_true_pred_ratings(df_final, pd.DataFrame(predicted_matrix, columns=R_df.columns, index=R_df.index))\n",
        "\n",
        "precision = precision_score(true_ratings, predicted_ratings.round(), average='weighted', zero_division=0)\n",
        "recall = recall_score(true_ratings, predicted_ratings.round(), average='weighted', zero_division=0)\n",
        "f1 = f1_score(true_ratings, predicted_ratings.round(), average='weighted', zero_division=0)\n",
        "accuracy = accuracy_score(true_ratings, predicted_ratings.round())\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dTrZ-FyU33q",
        "outputId": "6a37a73e-0b6a-42c0-b462-ff588ac9fa8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 10; error = nan\n",
            "Iteration: 20; error = nan\n",
            "Iteration: 30; error = nan\n",
            "Iteration: 40; error = nan\n",
            "Iteration: 50; error = nan\n",
            "Iteration: 60; error = nan\n",
            "Iteration: 70; error = nan\n",
            "Iteration: 80; error = nan\n",
            "Iteration: 90; error = nan\n",
            "Iteration: 100; error = nan\n",
            "[[4.46010186 3.60829331 3.43042852 ... 4.32580485 2.65673402 3.87182357]\n",
            " [4.14812481 3.34335054 3.14951091 ... 3.96385389 2.55786762 3.69507351]\n",
            " [4.27883658 3.44457187 3.23395441 ... 3.92351679 3.35727072 4.12188705]\n",
            " ...\n",
            " [4.46996054 4.18968623 3.91941883 ... 4.49668633 2.21048452 4.07896634]\n",
            " [4.19810411 3.50903944 3.27162849 ... 3.85862852 3.26002517 4.13844656]\n",
            " [3.44117644 1.93717885 1.8060465  ... 2.67968091 3.91627503 3.46236176]]\n",
            "Precision: 0.5177\n",
            "Recall: 0.4668\n",
            "F1 Score: 0.4472\n",
            "Accuracy: 0.4668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest old baseline, no class imbalance"
      ],
      "metadata": {
        "id": "eR7eYoVFBWlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import KFold, train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "BumORLoNBrKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load df Google Colab\n",
        "drive.mount('/content/drive')\n",
        "df_final = pd.read_csv(\"/content/drive/My Drive/Thesis/Data/df_final_2.csv\")\n",
        "\n",
        "df_final = df_final.astype(np.int32)"
      ],
      "metadata": {
        "id": "N2MCXRJIBnT-",
        "outputId": "89667536-4f40-46e6-f07c-19311e7629ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature and target\n",
        "X = df_final.drop(columns=['Rating'])\n",
        "y = df_final['Rating']\n",
        "\n",
        "# Split data 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    stratify=y, random_state=42)\n",
        "\n",
        "# Define the parameter grid for hyperparameter optimization\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 250, 300],\n",
        "    'max_depth': [20, 30, 40, 50],\n",
        "    'min_samples_split': [2,5,10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Define the inner and outer cross-validation strategies\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n"
      ],
      "metadata": {
        "id": "WGlPkz5LB17i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Randomized Search\n",
        "random_search = RandomizedSearchCV(estimator=RandomForestClassifier(),\n",
        "                                   param_distributions=param_dist, n_iter=15,\n",
        "                                   cv=inner_cv, scoring='accuracy')\n",
        "\n",
        "# Define output lists\n",
        "outer_scores = []\n",
        "val_scores = []\n",
        "best_params_list = []\n",
        "\n",
        "# Perform nested CV\n",
        "for train_idx, val_idx in outer_cv.split(X_train, y_train):\n",
        "    # Split the data into training and validation sets\n",
        "    X_train_fold = X_train.iloc[train_idx]\n",
        "    X_val_fold = X_train.iloc[val_idx]\n",
        "    y_train_fold = y_train.iloc[train_idx]\n",
        "    y_val_fold = y_train.iloc[val_idx]\n",
        "\n",
        "    # Fit the model on the training fold\n",
        "    random_search.fit(X_train_fold, y_train_fold)\n",
        "    best_model = random_search.best_estimator_\n",
        "\n",
        "    # Save the best parameters for this fold\n",
        "    best_params_list.append(random_search.best_params_)\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    val_predictions = best_model.predict(X_val_fold)\n",
        "    val_acc = accuracy_score(y_val_fold, val_predictions)\n",
        "    val_prec = precision_score(y_val_fold, val_predictions, average='weighted')\n",
        "    val_rec = recall_score(y_val_fold, val_predictions, average='weighted')\n",
        "    val_f1 = f1_score(y_val_fold, val_predictions, average='weighted')\n",
        "\n",
        "    # Store validation metrics\n",
        "    val_scores.append((val_acc, val_prec, val_rec, val_f1))\n",
        "\n",
        "    # Store the outer fold score (accuracy)\n",
        "    outer_scores.append(random_search.best_score_)\n",
        "\n",
        "# Print the performance of the nested CV\n",
        "print(f\"Nested Cross-Validation Accuracy: {np.mean(outer_scores):.2f} ± {np.std(outer_scores):.2f}\")\n",
        "\n",
        "# Print validation set performance for each fold\n",
        "val_scores = np.array(val_scores)\n",
        "print(\"Validation Set Performance:\")\n",
        "print(f\"Accuracy: {val_scores[:, 0].mean():.2f} ± {val_scores[:, 0].std():.2f}\")\n",
        "print(f\"Precision: {val_scores[:, 1].mean():.2f} ± {val_scores[:, 1].std():.2f}\")\n",
        "print(f\"Recall: {val_scores[:, 2].mean():.2f} ± {val_scores[:, 2].std():.2f}\")\n",
        "print(f\"F1 Score: {val_scores[:, 3].mean():.2f} ± {val_scores[:, 3].std():.2f}\")\n",
        "\n",
        "# Print the best parameters found during hyperparameter tuning\n",
        "print(\"Best parameters found during hyperparameter tuning:\")\n",
        "for i, params in enumerate(best_params_list):\n",
        "    print(f\"Fold {i+1}: {params}\")\n",
        "\n",
        "# Define the best parameters found in nested CV\n",
        "best_params = best_params_list[np.argmax(outer_scores)]\n",
        "\n",
        "# Train the best model\n",
        "best_model = RandomForestClassifier(**best_params, random_state=42)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_predictions = best_model.predict(X_test)\n",
        "print(\"Test set performance for best performing model:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, test_predictions):.2f}\")\n",
        "print(f\"Precision: {precision_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "print(f\"Recall: {recall_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "\n",
        "# Print the parameters for the best-performing model\n",
        "print(\"Parameters for the best performing model:\")\n",
        "print(best_params)\n",
        "\n",
        "# Save the best performing model\n",
        "joblib_file = \"/content/drive/My Drive/Thesis/Models/Baseline_RF.pkl\"\n",
        "joblib.dump(best_model, joblib_file)\n"
      ],
      "metadata": {
        "id": "_sVX3bf2CIDE",
        "outputId": "faed2314-91b5-42b3-e55f-b1e16346e98e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nested Cross-Validation Accuracy: 0.41 ± 0.00\n",
            "Validation Set Performance:\n",
            "Accuracy: 0.42 ± 0.00\n",
            "Precision: 0.42 ± 0.00\n",
            "Recall: 0.42 ± 0.00\n",
            "F1 Score: 0.39 ± 0.00\n",
            "Best parameters found during hyperparameter tuning:\n",
            "Fold 1: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 30}\n",
            "Fold 2: {'n_estimators': 250, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': 40}\n",
            "Fold 3: {'n_estimators': 300, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_depth': 40}\n",
            "Fold 4: {'n_estimators': 250, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': 40}\n",
            "Fold 5: {'n_estimators': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': 40}\n",
            "Test set performance for best performing model:\n",
            "Accuracy: 0.42\n",
            "Precision: 0.43\n",
            "Recall: 0.42\n",
            "F1 Score: 0.40\n",
            "Parameters for the best performing model:\n",
            "{'n_estimators': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': 40}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Thesis/Models/Baseline_RF.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGB with compute_sample_weight\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S0IX7MP0U2P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neT2J5_iVbRx",
        "outputId": "175755b8-5575-414a-f3ce-bd18c3d0e16f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.model_selection import KFold, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer"
      ],
      "metadata": {
        "id": "uBVQzo-ZVhzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the data\n",
        "drive.mount('/content/drive')\n",
        "df_final = pd.read_csv('/content/drive/My Drive/Thesis/Data/df_final_2.csv')\n",
        "\n",
        "df_final = df_final.astype(np.int32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ucHUxgBVpT9",
        "outputId": "78746426-718e-4a04-ee3a-58a13eb42828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the \"Rating\" column\n",
        "df_final['Rating'] = label_encoder.fit_transform(df_final['Rating'])"
      ],
      "metadata": {
        "id": "hN67n7ijVvN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, f1_score, accuracy_score, precision_score, recall_score\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from xgboost import XGBClassifier\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Define feature and target\n",
        "X = df_final.drop(columns=['Rating'])\n",
        "y = df_final['Rating']\n",
        "\n",
        "# Split data 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Compute sample weights\n",
        "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
        "\n",
        "# Define the parameter grid for tuning\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 400],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "# Define the inner and outer cross-validation strategies\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define the scoring metric\n",
        "f1_weighted_scorer = make_scorer(f1_score, average='weighted')\n",
        "\n",
        "# Initialize the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=XGBClassifier(tree_method='hist', device='cuda', eval_metric='merror'),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=15,\n",
        "    cv=inner_cv,\n",
        "    scoring=f1_weighted_scorer,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Define output lists\n",
        "outer_scores = []\n",
        "val_scores = []\n",
        "best_params_list = []\n",
        "\n",
        "# Perform nested CV on the training data\n",
        "for train_idx, val_idx in outer_cv.split(X_train, y_train):\n",
        "    # Split the data into training and validation sets\n",
        "    X_train_fold = X_train.iloc[train_idx]\n",
        "    X_val_fold = X_train.iloc[val_idx]\n",
        "    y_train_fold = y_train.iloc[train_idx]\n",
        "    y_val_fold = y_train.iloc[val_idx]\n",
        "    sample_weights_fold = sample_weights[train_idx]\n",
        "\n",
        "    # Fit the model on the training fold\n",
        "    random_search.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
        "    best_model = random_search.best_estimator_\n",
        "\n",
        "    # Save the best parameters for this fold\n",
        "    best_params_list.append(random_search.best_params_)\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    val_predictions = best_model.predict(X_val_fold)\n",
        "    val_acc = accuracy_score(y_val_fold, val_predictions)\n",
        "    val_prec = precision_score(y_val_fold, val_predictions, average='weighted')\n",
        "    val_rec = recall_score(y_val_fold, val_predictions, average='weighted')\n",
        "    val_f1 = f1_score(y_val_fold, val_predictions, average='weighted')\n",
        "\n",
        "    # Store validation metrics\n",
        "    val_scores.append((val_acc, val_prec, val_rec, val_f1))\n",
        "\n",
        "    # Store the outer fold score (f1_weighted)\n",
        "    outer_scores.append(random_search.best_score_)\n",
        "\n",
        "# Print the performance of the nested CV\n",
        "print(f\"Nested Cross-Validation F1 Score: {np.mean(outer_scores):.2f} ± {np.std(outer_scores):.2f}\")\n",
        "\n",
        "# Print validation set performance for each fold\n",
        "val_scores = np.array(val_scores)\n",
        "print(\"Validation Set Performance:\")\n",
        "print(f\"Accuracy: {val_scores[:, 0].mean():.2f} ± {val_scores[:, 0].std():.2f}\")\n",
        "print(f\"Precision: {val_scores[:, 1].mean():.2f} ± {val_scores[:, 1].std():.2f}\")\n",
        "print(f\"Recall: {val_scores[:, 2].mean():.2f} ± {val_scores[:, 2].std():.2f}\")\n",
        "print(f\"F1 Score: {val_scores[:, 3].mean():.2f} ± {val_scores[:, 3].std():.2f}\")\n",
        "\n",
        "# Print the best parameters found during hyperparameter tuning\n",
        "print(\"Best parameters found during hyperparameter tuning:\")\n",
        "for i, params in enumerate(best_params_list):\n",
        "    print(f\"Fold {i+1}: {params}\")\n",
        "\n",
        "# Define the best parameters found in nested CV\n",
        "best_params = best_params_list[np.argmax(outer_scores)]\n",
        "\n",
        "# Train best model on the entire training data with sample weights\n",
        "best_model = XGBClassifier(**best_params, tree_method='hist', device='cuda', eval_metric='merror', random_state=42)\n",
        "best_model.fit(X_train, y_train, sample_weight=compute_sample_weight(class_weight='balanced', y=y_train))\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_predictions = best_model.predict(X_test)\n",
        "print(\"Test set performance for best performing model:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, test_predictions):.2f}\")\n",
        "print(f\"Precision: {precision_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "print(f\"Recall: {recall_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "\n",
        "# Print the parameters for the best-performing model\n",
        "print(\"Parameters for the best performing model:\")\n",
        "print(best_params)\n",
        "\n",
        "# Save the best performing model\n",
        "joblib_file = \"/content/drive/My Drive/Thesis/Models/Baseline_XGB_class_imbalance.pkl\"\n",
        "joblib.dump(best_model, joblib_file)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeNaJKl2tJwU",
        "outputId": "97fbc7e1-92f6-4adf-ada3-d955fbf0855a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nested Cross-Validation F1 Score: 0.38 ± 0.00\n",
            "Validation Set Performance:\n",
            "Accuracy: 0.38 ± 0.00\n",
            "Precision: 0.41 ± 0.00\n",
            "Recall: 0.38 ± 0.00\n",
            "F1 Score: 0.38 ± 0.01\n",
            "Best parameters found during hyperparameter tuning:\n",
            "Fold 1: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Fold 2: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Fold 3: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Fold 4: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Fold 5: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Test set performance for best performing model:\n",
            "Accuracy: 0.39\n",
            "Precision: 0.42\n",
            "Recall: 0.39\n",
            "F1 Score: 0.39\n",
            "Parameters for the best performing model:\n",
            "{'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.2}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Thesis/Models/Baseline_XGB_class_imbalance.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVD"
      ],
      "metadata": {
        "id": "yxOkMMQLoDT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "uoiN9euLrqeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "df_final = pd.read_csv(\"/content/drive/My Drive/Thesis/Data/df_final_2.csv\")"
      ],
      "metadata": {
        "id": "vd1JrhNirCXV",
        "outputId": "6a4c4c01-3c58-49e0-d29d-d6e9f941ad52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 2.5% of the dataframe\n",
        "df = df_final.sample(frac=0.005, random_state=42)"
      ],
      "metadata": {
        "id": "L1HxSEmFra3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "BRMWgpIKsaZU",
        "outputId": "6d6408f7-deb1-44ad-c8b1-e4041830b209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        UserID  MovieID  Rating  Age  Year  Month  Day  Hour  Release_year  \\\n",
              "28385      824     1225       5    3  2000     11   28     3          1984   \n",
              "282334    1943     2879       3    1  2000     11   20     3          1990   \n",
              "312857    4605     3129       3    2  2000      7   21    18          1999   \n",
              "317108    3199     1210       4    1  2000      9   11     0          1983   \n",
              "321785    3367     3091       4    5  2000      9   27    18          1980   \n",
              "\n",
              "        Time_release_to_rating  ...  Favourite_Fantasy  Favourite_Film-Noir  \\\n",
              "28385                       16  ...              False                False   \n",
              "282334                      10  ...              False                False   \n",
              "312857                       1  ...              False                False   \n",
              "317108                      17  ...              False                False   \n",
              "321785                      20  ...              False                 True   \n",
              "\n",
              "        Favourite_Horror  Favourite_Musical  Favourite_Mystery  \\\n",
              "28385              False              False              False   \n",
              "282334             False              False              False   \n",
              "312857             False              False              False   \n",
              "317108             False              False              False   \n",
              "321785             False              False              False   \n",
              "\n",
              "        Favourite_Romance  Favourite_Sci-Fi  Favourite_Thriller  \\\n",
              "28385               False             False               False   \n",
              "282334              False             False               False   \n",
              "312857              False             False               False   \n",
              "317108              False             False               False   \n",
              "321785              False             False               False   \n",
              "\n",
              "        Favourite_War  Favourite_Western  \n",
              "28385            True              False  \n",
              "282334          False              False  \n",
              "312857          False               True  \n",
              "317108           True              False  \n",
              "321785          False              False  \n",
              "\n",
              "[5 rows x 71 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-73930d5a-e653-4704-828f-cdb5d19d3ce6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserID</th>\n",
              "      <th>MovieID</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Age</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>Day</th>\n",
              "      <th>Hour</th>\n",
              "      <th>Release_year</th>\n",
              "      <th>Time_release_to_rating</th>\n",
              "      <th>...</th>\n",
              "      <th>Favourite_Fantasy</th>\n",
              "      <th>Favourite_Film-Noir</th>\n",
              "      <th>Favourite_Horror</th>\n",
              "      <th>Favourite_Musical</th>\n",
              "      <th>Favourite_Mystery</th>\n",
              "      <th>Favourite_Romance</th>\n",
              "      <th>Favourite_Sci-Fi</th>\n",
              "      <th>Favourite_Thriller</th>\n",
              "      <th>Favourite_War</th>\n",
              "      <th>Favourite_Western</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>28385</th>\n",
              "      <td>824</td>\n",
              "      <td>1225</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2000</td>\n",
              "      <td>11</td>\n",
              "      <td>28</td>\n",
              "      <td>3</td>\n",
              "      <td>1984</td>\n",
              "      <td>16</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282334</th>\n",
              "      <td>1943</td>\n",
              "      <td>2879</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2000</td>\n",
              "      <td>11</td>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>1990</td>\n",
              "      <td>10</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312857</th>\n",
              "      <td>4605</td>\n",
              "      <td>3129</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2000</td>\n",
              "      <td>7</td>\n",
              "      <td>21</td>\n",
              "      <td>18</td>\n",
              "      <td>1999</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317108</th>\n",
              "      <td>3199</td>\n",
              "      <td>1210</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2000</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>1983</td>\n",
              "      <td>17</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321785</th>\n",
              "      <td>3367</td>\n",
              "      <td>3091</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2000</td>\n",
              "      <td>9</td>\n",
              "      <td>27</td>\n",
              "      <td>18</td>\n",
              "      <td>1980</td>\n",
              "      <td>20</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 71 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73930d5a-e653-4704-828f-cdb5d19d3ce6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-73930d5a-e653-4704-828f-cdb5d19d3ce6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-73930d5a-e653-4704-828f-cdb5d19d3ce6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2e629a0b-071f-4e2b-b3fa-be34fb7148cc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2e629a0b-071f-4e2b-b3fa-be34fb7148cc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2e629a0b-071f-4e2b-b3fa-be34fb7148cc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your dataframe with columns: UserID, MovieID, Rating\n",
        "n_users = df['UserID'].nunique()\n",
        "n_items = df['MovieID'].nunique()"
      ],
      "metadata": {
        "id": "JV85b_daru32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a user-item matrix\n",
        "user_item_matrix = df.pivot(index='UserID', columns='MovieID', values='Rating').fillna(0).to_numpy()\n"
      ],
      "metadata": {
        "id": "ChI5btihrznc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize parameters\n",
        "n_factors = 50  # Number of latent factors\n",
        "learning_rate = 0.01\n",
        "regularization = 0.1\n",
        "n_epochs = 20\n",
        "\n",
        "# Initialize user and item matrices\n",
        "U = np.random.normal(scale=1./n_factors, size=(n_users, n_factors))\n",
        "V = np.random.normal(scale=1./n_factors, size=(n_items, n_factors))\n",
        "\n",
        "# Indicator matrix for existing ratings\n",
        "indicator = (user_item_matrix > 0).astype(int)"
      ],
      "metadata": {
        "id": "UmNKJPTLr6Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with SGD\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(n_users):\n",
        "        for j in range(n_items):\n",
        "            if indicator[i, j] > 0:\n",
        "                # Compute the prediction error\n",
        "                prediction = np.dot(U[i, :], V[j, :])\n",
        "                error = user_item_matrix[i, j] - prediction\n",
        "\n",
        "                # Update user and item latent factors\n",
        "                U[i, :] += learning_rate * (error * V[j, :] - regularization * U[i, :])\n",
        "                V[j, :] += learning_rate * (error * U[i, :] - regularization * V[j, :])\n",
        "\n",
        "    # Compute the total loss (optional)\n",
        "    total_loss = np.sum((indicator * (user_item_matrix - np.dot(U, V.T)))**2) + regularization * (np.sum(U**2) + np.sum(V**2))\n",
        "    print(f'Epoch {epoch+1}/{n_epochs}, Loss: {total_loss}')\n",
        "\n",
        "# Predict the missing ratings\n",
        "predicted_ratings = np.dot(U, V.T)\n",
        "\n",
        "# Convert the predicted ratings to a DataFrame\n",
        "predicted_ratings_df = pd.DataFrame(predicted_ratings, columns=df['MovieID'].unique(), index=df['UserID'].unique())\n"
      ],
      "metadata": {
        "id": "HbPCtiUcq3QR",
        "outputId": "bfee1d43-a194-4fd2-c8a2-8166a0a0186c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 2910.2751329225407\n",
            "Epoch 2/20, Loss: 2907.877243140558\n",
            "Epoch 3/20, Loss: 2905.4452604807243\n",
            "Epoch 4/20, Loss: 2902.9555676678538\n",
            "Epoch 5/20, Loss: 2900.3839332504967\n",
            "Epoch 6/20, Loss: 2897.7052123159483\n",
            "Epoch 7/20, Loss: 2894.8930389775082\n",
            "Epoch 8/20, Loss: 2891.919507061959\n",
            "Epoch 9/20, Loss: 2888.754835794117\n",
            "Epoch 10/20, Loss: 2885.367017752993\n",
            "Epoch 11/20, Loss: 2881.7214470309086\n",
            "Epoch 12/20, Loss: 2877.7805264523954\n",
            "Epoch 13/20, Loss: 2873.503254017359\n",
            "Epoch 14/20, Loss: 2868.844790564453\n",
            "Epoch 15/20, Loss: 2863.7560131789264\n",
            "Epoch 16/20, Loss: 2858.183062296759\n",
            "Epoch 17/20, Loss: 2852.0668950077256\n",
            "Epoch 18/20, Loss: 2845.342862957065\n",
            "Epoch 19/20, Loss: 2837.9403406684273\n",
            "Epoch 20/20, Loss: 2829.7824391211293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Recommend 10 movies for user with UserID 1\n",
        "user_id = 824\n",
        "user_ratings = predicted_ratings_df.loc[user_id].sort_values(ascending=False)\n",
        "print(user_ratings.head(10))"
      ],
      "metadata": {
        "id": "NOcK5oxnsfL3",
        "outputId": "b81bdd58-dc39-4aff-f0ad-d846aa571412",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2694    0.025831\n",
            "2193    0.016504\n",
            "1266    0.016426\n",
            "1196    0.014798\n",
            "3404    0.012192\n",
            "1810    0.011734\n",
            "260     0.011194\n",
            "923     0.010700\n",
            "3481    0.010421\n",
            "2066    0.009654\n",
            "Name: 824, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate RMSE\n",
        "def calculate_rmse(actual, predicted, indicator):\n",
        "    # Only consider the actual ratings (where indicator is 1)\n",
        "    error = indicator * (actual - predicted)\n",
        "    mse = np.sum(error**2) / np.sum(indicator)\n",
        "    rmse = np.sqrt(mse)\n",
        "    return rmse\n",
        "\n",
        "# Actual ratings matrix (user-item matrix)\n",
        "actual_ratings = user_item_matrix\n",
        "\n",
        "# Predicted ratings matrix\n",
        "predicted_ratings = np.dot(U, V.T)\n",
        "\n",
        "# Indicator matrix for existing ratings\n",
        "indicator = (actual_ratings > 0).astype(int)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = calculate_rmse(actual_ratings, predicted_ratings, indicator)\n",
        "print(f'RMSE: {rmse}')"
      ],
      "metadata": {
        "id": "yxdCSpNXs9Ec",
        "outputId": "92a9b39a-8657-496a-d806-90316fa0c3d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 3.7601852611228685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter optimized SVD"
      ],
      "metadata": {
        "id": "hU73dfEitqxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "# Assuming df is your dataframe with columns: UserID, MovieID, Rating\n",
        "n_users = df['UserID'].nunique()\n",
        "n_items = df['MovieID'].nunique()\n",
        "\n",
        "# Create a user-item matrix\n",
        "user_item_matrix = df.pivot(index='UserID', columns='MovieID', values='Rating').fillna(0).to_numpy()\n",
        "\n",
        "# Hyperparameters to optimize\n",
        "n_factors_list = [10, 20, 50]\n",
        "learning_rate_list = [0.001, 0.01, 0.1]\n",
        "regularization_list = [0.01, 0.1, 1]\n",
        "\n",
        "# K-Fold Cross Validation\n",
        "outer_kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "inner_kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "best_params = None\n",
        "best_rmse = float('inf')\n",
        "\n",
        "for train_index, test_index in outer_kf.split(user_item_matrix):\n",
        "    train_data = user_item_matrix[train_index]\n",
        "    test_data = user_item_matrix[test_index]\n",
        "\n",
        "    for n_factors in n_factors_list:\n",
        "        for learning_rate in learning_rate_list:\n",
        "            for regularization in regularization_list:\n",
        "                inner_rmse = []\n",
        "\n",
        "                for inner_train_index, inner_val_index in inner_kf.split(train_data):\n",
        "                    inner_train_data = train_data[inner_train_index]\n",
        "                    inner_val_data = train_data[inner_val_index]\n",
        "\n",
        "                    # Initialize user and item matrices\n",
        "                    U = np.random.normal(scale=1./n_factors, size=(n_users, n_factors))\n",
        "                    V = np.random.normal(scale=1./n_factors, size=(n_items, n_factors))\n",
        "\n",
        "                    # Indicator matrix for existing ratings\n",
        "                    indicator_train = (inner_train_data > 0).astype(int)\n",
        "                    indicator_val = (inner_val_data > 0).astype(int)\n",
        "\n",
        "                    # Training with SGD\n",
        "                    for epoch in range(20):\n",
        "                        for i in range(n_users):\n",
        "                            for j in range(n_items):\n",
        "                                if indicator_train[i, j] > 0:\n",
        "                                    # Compute the prediction error\n",
        "                                    prediction = np.dot(U[i, :], V[j, :])\n",
        "                                    error = inner_train_data[i, j] - prediction\n",
        "\n",
        "                                    # Update user and item latent factors\n",
        "                                    U[i, :] += learning_rate * (error * V[j, :] - regularization * U[i, :])\n",
        "                                    V[j, :] += learning_rate * (error * U[i, :] - regularization * V[j, :])\n",
        "\n",
        "                    # Predict the validation ratings\n",
        "                    predicted_val_ratings = np.dot(U, V.T)\n",
        "\n",
        "                    # Calculate RMSE for validation set\n",
        "                    val_rmse = np.sqrt(mean_squared_error(inner_val_data[indicator_val > 0], predicted_val_ratings[indicator_val > 0]))\n",
        "                    inner_rmse.append(val_rmse)\n",
        "\n",
        "                avg_inner_rmse = np.mean(inner_rmse)\n",
        "\n",
        "                if avg_inner_rmse < best_rmse:\n",
        "                    best_rmse = avg_inner_rmse\n",
        "                    best_params = (n_factors, learning_rate, regularization)\n",
        "\n",
        "# Train the final model with the best hyperparameters on the entire training set\n",
        "n_factors, learning_rate, regularization = best_params\n",
        "\n",
        "U_final = np.random.normal(scale=1./n_factors, size=(n_users, n_factors))\n",
        "V_final = np.random.normal(scale=1./n_factors, size=(n_items, n_factors))\n",
        "\n",
        "indicator_train_final = (train_data > 0).astype(int)\n",
        "\n",
        "for epoch in range(20):\n",
        "    for i in range(n_users):\n",
        "        for j in range(n_items):\n",
        "            if indicator_train_final[i, j] > 0:\n",
        "                prediction = np.dot(U_final[i, :], V_final[j, :])\n",
        "                error = train_data[i, j] - prediction\n",
        "\n",
        "                U_final[i, :] += learning_rate * (error * V_final[j, :] - regularization * U_final[i, :])\n",
        "                V_final[j, :] += learning_rate * (error * U_final[i, :] - regularization * V_final[i, :])\n",
        "\n",
        "# Predict the test ratings\n",
        "predicted_test_ratings = np.dot(U_final, V_final.T)\n",
        "\n",
        "# Calculate RMSE for test set\n",
        "indicator_test = (test_data > 0).astype(int)\n",
        "test_rmse = np.sqrt(mean_squared_error(test_data[indicator_test > 0], predicted_test_ratings[indicator_test > 0]))\n",
        "\n",
        "print(f'Best Hyperparameters: n_factors={n_factors}, learning_rate={learning_rate}, regularization={regularization}')\n",
        "print(f'Test RMSE: {test_rmse}')"
      ],
      "metadata": {
        "id": "f9y1M37Stvlq",
        "outputId": "f4607680-2ec5-4b48-d9a3-d1f791b910b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 774 is out of bounds for axis 0 with size 774",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b6bf84fea6a8>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                                 \u001b[0;32mif\u001b[0m \u001b[0mindicator_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                                     \u001b[0;31m# Compute the prediction error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                                     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 774 is out of bounds for axis 0 with size 774"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "# Assuming df is your dataframe with columns: UserID, MovieID, Rating\n",
        "n_users = df['UserID'].nunique()\n",
        "n_items = df['MovieID'].nunique()\n",
        "\n",
        "# Create a user-item matrix\n",
        "user_item_matrix = df.pivot(index='UserID', columns='MovieID', values='Rating').fillna(0).to_numpy()\n",
        "\n",
        "# Hyperparameters to optimize\n",
        "n_factors_list = [10, 20, 50]\n",
        "learning_rate_list = [0.001, 0.01, 0.1]\n",
        "regularization_list = [0.01, 0.1, 1]\n",
        "\n",
        "# K-Fold Cross Validation\n",
        "outer_kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "inner_kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "best_params = None\n",
        "best_rmse = float('inf')\n",
        "\n",
        "for train_index, test_index in outer_kf.split(user_item_matrix):\n",
        "    train_data = user_item_matrix[train_index]\n",
        "    test_data = user_item_matrix[test_index]\n",
        "\n",
        "    for n_factors in n_factors_list:\n",
        "        for learning_rate in learning_rate_list:\n",
        "            for regularization in regularization_list:\n",
        "                inner_rmse = []\n",
        "\n",
        "                for inner_train_index, inner_val_index in inner_kf.split(train_data):\n",
        "                    inner_train_data = train_data[inner_train_index]\n",
        "                    inner_val_data = train_data[inner_val_index]\n",
        "\n",
        "                    # Initialize user and item matrices\n",
        "                    U = np.random.normal(scale=1./n_factors, size=(inner_train_data.shape[0], n_factors))\n",
        "                    V = np.random.normal(scale=1./n_factors, size=(inner_train_data.shape[1], n_factors))\n",
        "\n",
        "                    # Indicator matrix for existing ratings\n",
        "                    indicator_train = (inner_train_data > 0).astype(int)\n",
        "                    indicator_val = (inner_val_data > 0).astype(int)\n",
        "\n",
        "                    # Training with SGD\n",
        "                    for epoch in range(20):\n",
        "                        for i in range(inner_train_data.shape[0]):\n",
        "                            for j in range(inner_train_data.shape[1]):\n",
        "                                if indicator_train[i, j] > 0:\n",
        "                                    # Compute the prediction error\n",
        "                                    prediction = np.dot(U[i, :], V[j, :])\n",
        "                                    error = inner_train_data[i, j] - prediction\n",
        "\n",
        "                                    # Update user and item latent factors\n",
        "                                    U[i, :] += learning_rate * (error * V[j, :] - regularization * U[i, :])\n",
        "                                    V[j, :] += learning_rate * (error * U[i, :] - regularization * V[j, :])\n",
        "\n",
        "                    # Predict the validation ratings\n",
        "                    predicted_val_ratings = np.dot(U, V.T)\n",
        "\n",
        "                    # Calculate RMSE for validation set\n",
        "                    val_rmse = np.sqrt(mean_squared_error(inner_val_data[indicator_val > 0], predicted_val_ratings[indicator_val > 0]))\n",
        "                    inner_rmse.append(val_rmse)\n",
        "\n",
        "                avg_inner_rmse = np.mean(inner_rmse)\n",
        "\n",
        "                if avg_inner_rmse < best_rmse:\n",
        "                    best_rmse = avg_inner_rmse\n",
        "                    best_params = (n_factors, learning_rate, regularization)\n",
        "\n",
        "# Train the final model with the best hyperparameters on the entire training set\n",
        "n_factors, learning_rate, regularization = best_params\n",
        "\n",
        "U_final = np.random.normal(scale=1./n_factors, size=(train_data.shape[0], n_factors))\n",
        "V_final = np.random.normal(scale=1./n_factors, size=(train_data.shape[1], n_factors))\n",
        "\n",
        "indicator_train_final = (train_data > 0).astype(int)\n",
        "\n",
        "for epoch in range(20):\n",
        "    for i in range(train_data.shape[0]):\n",
        "        for j in range(train_data.shape[1]):\n",
        "            if indicator_train_final[i, j] > 0:\n",
        "                prediction = np.dot(U_final[i, :], V_final[j, :])\n",
        "                error = train_data[i, j] - prediction\n",
        "\n",
        "                U_final[i, :] += learning_rate * (error * V_final[j, :] - regularization * U_final[i, :])\n",
        "                V_final[j, :] += learning_rate * (error * U_final[i, :] - regularization * V_final[j, :])\n",
        "\n",
        "# Predict the test ratings\n",
        "predicted_test_ratings = np.dot(U_final, V_final.T)\n",
        "\n",
        "# Calculate RMSE for test set\n",
        "indicator_test = (test_data > 0).astype(int)\n",
        "test_rmse = np.sqrt(mean_squared_error(test_data[indicator_test > 0], predicted_test_ratings[indicator_test > 0]))\n",
        "\n",
        "print(f'Best Hyperparameters: n_factors={n_factors}, learning_rate={learning_rate}, regularization={regularization}')\n",
        "print(f'Test RMSE: {test_rmse}')"
      ],
      "metadata": {
        "id": "sbAlk0MmvMl4",
        "outputId": "bce47205-da00-44f8-cb75-52a85c3c0b1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "boolean index did not match indexed array along dimension 0; dimension is 774 but corresponding boolean dimension is 387",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-dc66ee248c79>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0;31m# Calculate RMSE for validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                     \u001b[0mval_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_val_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindicator_val\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_val_ratings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindicator_val\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                     \u001b[0minner_rmse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_rmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 774 but corresponding boolean dimension is 387"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation: The user-item matrix is created from the dataframe.\n",
        "Hyperparameter Lists: Lists of hyperparameters to be optimized.\n",
        "K-Fold Cross Validation: Outer and inner k-fold cross-validation splits.\n",
        "Training with SGD: The user and item matrices are updated iteratively to minimize the loss function, which includes a regularization term to prevent overfitting.\n",
        "Hyperparameter Optimization: The best hyperparameters are selected based on the average RMSE from the inner folds.\n",
        "Final Model Training: The final model is trained on the entire training set using the best hyperparameters.\n",
        "Evaluation: The RMSE is calculated on the test set to evaluate the model's performance."
      ],
      "metadata": {
        "id": "2ZLX3I4Mtwt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "import joblib\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def evaluate_model(model, X, y, dataset_name=\"\"):\n",
        "    predictions = model.predict(X)\n",
        "    acc = accuracy_score(y, predictions)\n",
        "    prec = precision_score(y, predictions, average='weighted')\n",
        "    rec = recall_score(y, predictions, average='weighted')\n",
        "    f1 = f1_score(y, predictions, average='weighted')\n",
        "\n",
        "    print(f\"{dataset_name} Set Performance:\")\n",
        "    print(f\"Accuracy: {acc:.2f}\")\n",
        "    print(f\"Precision: {prec:.2f}\")\n",
        "    print(f\"Recall: {rec:.2f}\")\n",
        "    print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "    return acc, prec, rec, f1\n",
        "\n",
        "# Initialize the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=XGBClassifier(tree_method='hist', device='cuda', eval_metric='merror'),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=15,\n",
        "    cv=inner_cv,\n",
        "    scoring=f1_weighted_scorer,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Define output lists\n",
        "outer_scores = []\n",
        "val_scores = []\n",
        "train_scores = []\n",
        "best_params_list = []\n",
        "all_param_scores = []\n",
        "\n",
        "# Perform nested CV on the training data\n",
        "for train_idx, val_idx in outer_cv.split(X_train, y_train):\n",
        "    # Split the data into training and validation sets\n",
        "    X_train_fold = X_train.iloc[train_idx]\n",
        "    X_val_fold = X_train.iloc[val_idx]\n",
        "    y_train_fold = y_train.iloc[train_idx]\n",
        "    y_val_fold = y_train.iloc[val_idx]\n",
        "\n",
        "    # Fit the model on the training fold\n",
        "    random_search.fit(X_train_fold, y_train_fold)\n",
        "    best_model = random_search.best_estimator_\n",
        "\n",
        "    # Save the best parameters for this fold\n",
        "    best_params_list.append(random_search.best_params_)\n",
        "\n",
        "    # Evaluate on the training set\n",
        "    train_scores.append(evaluate_model(best_model, X_train_fold, y_train_fold, \"Training\"))\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    val_scores.append(evaluate_model(best_model, X_val_fold, y_val_fold, \"Validation\"))\n",
        "\n",
        "    # Store the outer fold score (f1_weighted)\n",
        "    outer_scores.append(random_search.best_score_)\n",
        "\n",
        "    # Store all parameter configurations and their scores\n",
        "    all_param_scores.append(random_search.cv_results_['mean_test_score'])\n",
        "\n",
        "# Print the performance of the nested CV\n",
        "print(f\"Nested Cross-Validation F1 Score: {np.mean(outer_scores):.2f} ± {np.std(outer_scores):.2f}\")\n",
        "\n",
        "# Print training set performance for each fold\n",
        "train_scores = np.array(train_scores)\n",
        "print(\"Training Set Performance (Averaged):\")\n",
        "print(f\"Accuracy: {train_scores[:, 0].mean():.2f} ± {train_scores[:, 0].std():.2f}\")\n",
        "print(f\"Precision: {train_scores[:, 1].mean():.2f} ± {train_scores[:, 1].std():.2f}\")\n",
        "print(f\"Recall: {train_scores[:, 2].mean():.2f} ± {train_scores[:, 2].std():.2f}\")\n",
        "print(f\"F1 Score: {train_scores[:, 3].mean():.2f} ± {train_scores[:, 3].std():.2f}\")\n",
        "\n",
        "# Print validation set performance for each fold\n",
        "val_scores = np.array(val_scores)\n",
        "print(\"Validation Set Performance (Averaged):\")\n",
        "print(f\"Accuracy: {val_scores[:, 0].mean():.2f} ± {val_scores[:, 0].std():.2f}\")\n",
        "print(f\"Precision: {val_scores[:, 1].mean():.2f} ± {val_scores[:, 1].std():.2f}\")\n",
        "print(f\"Recall: {val_scores[:, 2].mean():.2f} ± {val_scores[:, 2].std():.2f}\")\n",
        "print(f\"F1 Score: {val_scores[:, 3].mean():.2f} ± {val_scores[:, 3].std():.2f}\")\n",
        "\n",
        "# Print the best parameters found during hyperparameter tuning\n",
        "print(\"Best parameters found during hyperparameter tuning:\")\n",
        "for i, params in enumerate(best_params_list):\n",
        "    print(f\"Fold {i+1}: {params}\")\n",
        "\n",
        "# Print F1 scores of different parameter configurations tried during hyperparameter tuning\n",
        "print(\"F1 scores of different parameter configurations tried during hyperparameter tuning:\")\n",
        "for i, param_scores in enumerate(all_param_scores):\n",
        "    print(f\"Fold {i+1}: {param_scores}\")\n",
        "\n",
        "# Define the best parameters found in nested CV\n",
        "best_params = best_params_list[np.argmax(outer_scores)]\n",
        "\n",
        "# Train best model on the entire training data\n",
        "best_model = XGBClassifier(**best_params, tree_method='hist', device='cuda', eval_metric='merror', random_state=42)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on the training set\n",
        "evaluate_model(best_model, X_train, y_train, \"Training\")\n",
        "\n",
        "# Evaluate on the test set\n",
        "evaluate_model(best_model, X_test, y_test, \"Test\")\n",
        "\n",
        "# Print the parameters for the best-performing model\n",
        "print(\"Parameters for the best performing model:\")\n",
        "print(best_params)\n",
        "\n",
        "joblib.dump(best_model, \"/content/drive/My Drive/Thesis/Models/Baseline_XGB_training_info.pkl\")"
      ],
      "metadata": {
        "id": "XhCppO6zUcD7",
        "outputId": "b80f4c18-a813-4ef8-ccb8-da0593dc88db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [08:12:28] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "Potential solutions:\n",
            "- Use a data structure that matches the device ordinal in the booster.\n",
            "- Set the device for booster before call to inplace_predict.\n",
            "\n",
            "This warning will only be shown once.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nested Cross-Validation F1 Score: 0.41 ± 0.00\n",
            "Validation Set Performance (All Configurations):\n",
            "Accuracy: 0.44 ± 0.00\n",
            "F1 Score: 0.42 ± 0.01\n",
            "Validation Set Performance (Best Model):\n",
            "Accuracy: 0.44 ± 0.00\n",
            "F1 Score: 0.42 ± 0.01\n",
            "Best parameters found during hyperparameter tuning:\n",
            "Fold 1: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Fold 2: {'n_estimators': 400, 'max_depth': 5, 'learning_rate': 0.2}\n",
            "Fold 3: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.1}\n",
            "Fold 4: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Fold 5: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Training set performance for best performing model:\n",
            "Accuracy: 0.61\n",
            "F1 Score: 0.60\n",
            "Test set performance for best performing model:\n",
            "Accuracy: 0.44\n",
            "F1 Score: 0.43\n",
            "Parameters for the best performing model:\n",
            "{'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.2}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/Thesis/Models/Baseline_XGB_25/11.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8df31518aa4a>\u001b[0m in \u001b[0;36m<cell line: 152>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;31m# Save the best performing model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0mjoblib_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/Thesis/Models/Baseline_XGB_25/11.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoblib_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mNumpyPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_filename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0mNumpyPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Thesis/Models/Baseline_XGB_25/11.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **XGBoost baseline zonder training set**"
      ],
      "metadata": {
        "id": "QDKqZQqSwkPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xgboost"
      ],
      "metadata": {
        "id": "Z61n3CeTwnFp",
        "outputId": "22ee0f30-e079-4539-a33f-aafd78fe39c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "from sklearn.model_selection import KFold, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer"
      ],
      "metadata": {
        "id": "zc5O8afiwrJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the data\n",
        "drive.mount('/content/drive')\n",
        "df_final = pd.read_csv('/content/drive/My Drive/Thesis/Data/df_final_2.csv')\n",
        "\n",
        "df_final = df_final.astype(np.int32)"
      ],
      "metadata": {
        "id": "Aia4WYx_wtHL",
        "outputId": "4cb3e4d1-9cbd-428b-830c-b03fb8fdb404",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the \"Rating\" column\n",
        "df_final['Rating'] = label_encoder.fit_transform(df_final['Rating'])"
      ],
      "metadata": {
        "id": "0ZRqPQkjw0oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature and target\n",
        "X = df_final.drop(columns=['Rating'])\n",
        "y = df_final['Rating']\n",
        "\n",
        "# Split data 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    stratify=y, random_state=42)\n",
        "\n",
        "# Define the parameter grid for tuning\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 400],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Define the inner and outer cross-validation strategies\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define the scoring metric\n",
        "f1_weighted_scorer = make_scorer(f1_score, average='weighted')"
      ],
      "metadata": {
        "id": "deNv3Xqzw49H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=XGBClassifier(tree_method='hist', device='cuda',\n",
        "                            eval_metric='merror'),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=15,\n",
        "    cv=inner_cv,\n",
        "    scoring=f1_weighted_scorer,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Define output lists\n",
        "outer_scores = []\n",
        "val_scores = []\n",
        "best_params_list = []\n",
        "\n",
        "# Perform nested CV on the training data\n",
        "for train_idx, val_idx in outer_cv.split(X_train, y_train):\n",
        "    # Split the data into training and validation sets\n",
        "    X_train_fold = X_train.iloc[train_idx]\n",
        "    X_val_fold = X_train.iloc[val_idx]\n",
        "    y_train_fold = y_train.iloc[train_idx]\n",
        "    y_val_fold = y_train.iloc[val_idx]\n",
        "\n",
        "    # Fit the model on the training fold\n",
        "    random_search.fit(X_train_fold, y_train_fold)\n",
        "    best_model = random_search.best_estimator_\n",
        "\n",
        "    # Save the best parameters for this fold\n",
        "    best_params_list.append(random_search.best_params_)\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    val_predictions = best_model.predict(X_val_fold)\n",
        "    val_acc = accuracy_score(y_val_fold, val_predictions)\n",
        "    val_prec = precision_score(y_val_fold, val_predictions, average='weighted')\n",
        "    val_rec = recall_score(y_val_fold, val_predictions, average='weighted')\n",
        "    val_f1 = f1_score(y_val_fold, val_predictions, average='weighted')\n",
        "\n",
        "    # Store validation metrics\n",
        "    val_scores.append((val_acc, val_prec, val_rec, val_f1))\n",
        "\n",
        "    # Store the outer fold score (f1_weighted)\n",
        "    outer_scores.append(random_search.best_score_)\n",
        "\n",
        "# Print the performance of the nested CV\n",
        "print(f\"Nested Cross-Validation F1 Score: {np.mean(outer_scores):.2f} ± {np.std(outer_scores):.2f}\")\n",
        "\n",
        "# Print validation set performance for each fold\n",
        "val_scores = np.array(val_scores)\n",
        "print(\"Validation Set Performance:\")\n",
        "print(f\"Accuracy: {val_scores[:, 0].mean():.2f} ± {val_scores[:, 0].std():.2f}\")\n",
        "print(f\"Precision: {val_scores[:, 1].mean():.2f} ± {val_scores[:, 1].std():.2f}\")\n",
        "print(f\"Recall: {val_scores[:, 2].mean():.2f} ± {val_scores[:, 2].std():.2f}\")\n",
        "print(f\"F1 Score: {val_scores[:, 3].mean():.2f} ± {val_scores[:, 3].std():.2f}\")\n",
        "\n",
        "# Print the best parameters found during hyperparameter tuning\n",
        "print(\"Best parameters found during hyperparameter tuning:\")\n",
        "for i, params in enumerate(best_params_list):\n",
        "    print(f\"Fold {i+1}: {params}\")\n",
        "\n",
        "# Define the best parameters found in nested CV\n",
        "best_params = best_params_list[np.argmax(outer_scores)]\n",
        "\n",
        "# Train best model on the entire training data\n",
        "best_model = XGBClassifier(**best_params, tree_method='hist',\n",
        "                           device='cuda', eval_metric='merror',\n",
        "                           random_state=42)\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_predictions = best_model.predict(X_test)\n",
        "print(\"Test set performance for best performing model:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, test_predictions):.2f}\")\n",
        "print(f\"Precision: {precision_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "print(f\"Recall: {recall_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, test_predictions, average='weighted'):.2f}\")\n",
        "\n",
        "# Print the parameters for the best-performing model\n",
        "print(\"Parameters for the best performing model:\")\n",
        "print(best_params)\n",
        "\n",
        "# Save the best performing model\n",
        "joblib_file = \"/content/drive/My Drive/Thesis/Models/Baseline_XGB.pkl\"\n",
        "joblib.dump(best_model, joblib_file)\n"
      ],
      "metadata": {
        "id": "R9X8MN1Aw7P-",
        "outputId": "57987401-9d19-4846-ae34-b415b6aa77ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
            "  _data = np.array(data, dtype=dtype, copy=copy,\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [11:46:48] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "Potential solutions:\n",
            "- Use a data structure that matches the device ordinal in the booster.\n",
            "- Set the device for booster before call to inplace_predict.\n",
            "\n",
            "This warning will only be shown once.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nested Cross-Validation F1 Score: 0.41 ± 0.00\n",
            "Validation Set Performance:\n",
            "Accuracy: 0.44 ± 0.00\n",
            "Precision: 0.43 ± 0.00\n",
            "Recall: 0.44 ± 0.00\n",
            "F1 Score: 0.42 ± 0.00\n",
            "Best parameters found during hyperparameter tuning:\n",
            "Fold 1: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Fold 2: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.1}\n",
            "Fold 3: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Fold 4: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.1}\n",
            "Fold 5: {'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.2}\n",
            "Test set performance for best performing model:\n",
            "Accuracy: 0.44\n",
            "Precision: 0.44\n",
            "Recall: 0.44\n",
            "F1 Score: 0.43\n",
            "Parameters for the best performing model:\n",
            "{'n_estimators': 400, 'max_depth': 7, 'learning_rate': 0.2}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Thesis/Models/Baseline_XGB.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}